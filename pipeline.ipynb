{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to Train and Test Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths & Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, exists\n",
    "from paths import RETOUCH_PATH, IMAGES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train-Test Split for the Fluid Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.folds_split import k_fold_split_segmentation\n",
    "\n",
    "k = 5\n",
    "\n",
    "if not (isfile(path=\"splits\\segmentation_train_splits.csv\") or isfile(path=\"splits\\segmentation_test_splits.csv\")):\n",
    "    k_fold_split_segmentation(k=k, folders_path=RETOUCH_PATH)\n",
    "else:\n",
    "    print(\"Split already exists. To create a new one please delete the old files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images Reading and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCT Volumes Reading and Saving for Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.read_oct import save_segmentation_oct_as_tiff\n",
    "\n",
    "if not ((exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\slices\\\\int32\")) and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\slices\\\\uint8\"))):\n",
    "    save_segmentation_oct_as_tiff(oct_folder=RETOUCH_PATH, save_folder=IMAGES_PATH)\n",
    "else:\n",
    "    print(\"Images have already been extracted. To extract them again, please delete the folder with the images.\")\n",
    "\n",
    "# ETA: 2m29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCT Masks Reading and Saving for Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.read_oct import save_segmentation_mask_as_tiff\n",
    "\n",
    "if not ((exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\masks\\\\int8\")) and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\masks\\\\uint8\"))):\n",
    "    save_segmentation_mask_as_tiff(oct_folder=RETOUCH_PATH, save_folder=IMAGES_PATH)\n",
    "else:\n",
    "    print(\"Masks have already been extracted. To extract them again, please delete the folder with the images.\")\n",
    "\n",
    "# ETA: 3m4.4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROI Masks Extraction for Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.patch_extraction import extractROIMasks\n",
    "\n",
    "if not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\slices\\\\int32\") and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\slices\\\\uint8\"))):\n",
    "    print(\"First, the images must be extracted from the OCT volumes.\")\n",
    "elif not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\masks\\\\int8\") and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\masks\\\\uint8\"))):\n",
    "    print(\"First, the masks must be extracted from the OCT volumes.\")\n",
    "elif not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\roi\")):\n",
    "    extractROIMasks(oct_path=RETOUCH_PATH , folder_path=IMAGES_PATH, threshold=1e-2)\n",
    "else:\n",
    "    print(\"Patches have already been extracted. To extract them again, please delete the folder that contains the extracted ROI masks.\")\n",
    "\n",
    "# ETA: 8h46m01s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Extraction and Saving\n",
    "\n",
    "Not required to run the project, just to check what is being done and if it is being done correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patches Extraction for 2D Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.patch_extraction import extractPatches\n",
    "\n",
    "if not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\roi\\\\int8\") and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\roi\\\\uint8\"))):\n",
    "    print(\"First, the ROI masks must be extracted from the OCT volumes.\")\n",
    "elif not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\patches\\\\2D\")):\n",
    "    extractPatches(IMAGES_PATH, patch_shape=(256,128), n_pos=12, n_neg=2, pos=1, neg=0)\n",
    "else:\n",
    "    print(\"Patches have already been extracted. To extract them again, please delete the folder that contains the extracted patches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patches Extraction for 2.5D Segmentation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.patch_extraction import extractPatches25D\n",
    "\n",
    "if not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\roi\\\\int8\") and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\roi\\\\uint8\"))):\n",
    "    print(\"First, the ROI masks must be extracted from the OCT volumes.\")\n",
    "elif not (exists(IMAGES_PATH + \"\\\\OCT_images\\\\segmentation\\\\patches\\\\2.5D\")):\n",
    "    extractPatches25D(IMAGES_PATH, patch_shape=(256,128), n_pos=12, n_neg=2, pos=1, neg=0)\n",
    "else:\n",
    "    print(\"Patches have already been extracted. To extract them again, please delete the folder that contains the extracted patches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 1*\n",
    "- Epochs: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run1\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "The first run. Due to a logic error, the best model was not saved and early stopping was not possible. However, it is interesting to understand the behavior of the U-Net in this conditions. While training loss slowly goes down to 0.26, the validation loss never crosses below the 0.39. Perhaps, a learning rate scheduler would be useful to implement in this conditions, to check if lower validation score is possible. Similarly, implementing this experiment with a lower learning rate may be useful to find a possible lower minimum error, with the expense of requiring a larger number of epochs in training. Also, it is important to mention that in this experiment, the data was obtained assycronously, unlike what was initially proposed, that recommended extracting random patches in every epoch. This later implementation must be compared against the assyncronous, to evaluate the necessaty of the patch extraction every epoch. It is important to note that the patch extraction is very time consuming, with the training process in an epoch taking as long as 15 minutes (without it, it takes 2 minutes).\n",
    "\n",
    "![Training Error in Run1](./imgs/Run1_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run1_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run1_slice_dice.csv), [vendor](results/Run1_vendor_dice.csv), [volume](results/Run1_volume_dice.csv), and [class](results/Run1_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.458  |  0.307  |  0.390  |\n",
    "| Spectralis |  0.644  |  0.725  |  0.494  |\n",
    "| Topcon     |  0.356  |  0.475  |  0.403  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.484  |  0.527  |  0.435  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 2*\n",
    "- Epochs: 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run1\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "In this run, the number of epochs was increased from 100 to 200, in order to verify whether it is possible to obtain better results in a longer run with the previous hyper parameters. The best model obtained in this training performs really similar to the previous, taking twice the same amount of time.\n",
    "\n",
    "![Training Error in Run2](./imgs/Run2_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run2_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run2_slice_dice.csv), [vendor](results/Run2_vendor_dice.csv), [volume](results/Run2_volume_dice.csv), and [class](results/Run2_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.394  |  0.235  |  0.343  |\n",
    "| Spectralis |  0.609  |  0.684  |  0.425  |\n",
    "| Topcon     |  0.438  |  0.423  |  0.415  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.477  |  0.472  |  0.385  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 3*\n",
    "- Patch extraction was done every epoch without patch dropping, instead of once before the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run3\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=False,\n",
    "    patch_dropping=False,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "The model was trained for a longer period of time, since it required patch extraction before training each epoch. Overall, an improvement in the Dice score was seen across all the vendors and classes, especially in the SRF class. However, it requires an increased amount of training time since larger batches are considered (without dropping patches) and the extraction of images makes the process longer.\n",
    "\n",
    "![Training Error in Run3](./imgs/Run3_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run3_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run3_slice_dice.csv), [vendor](results/Run3_vendor_dice.csv), [volume](results/Run3_volume_dice.csv), and [class](results/Run3_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.456  |  0.296  |  0.301  |\n",
    "| Spectralis |  0.684  |  0.763  |  0.489  |\n",
    "| Topcon     |  0.322  |  0.525  |  0.468  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.485  |  0.543  |  0.398  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 4*\n",
    "- Syncronous patch extraction with patch dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run4\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=False,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "The results obtained regarding the loss during training and validation were really similar to those obtained with assyncronous patch extraction. This begs the question whether the improvements in last training were due to the randomization that patch extraction done in every epoch brings or due to the larger quantity of data available due to no employing patch dropout. One future experiment may be exactly that: re-run the first experiment without patch dropout. \n",
    "\n",
    "![Training Error in Run4](./imgs/Run4_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run4_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run4_slice_dice.csv), [vendor](results/Run4_vendor_dice.csv), [volume](results/Run4_volume_dice.csv), and [class](results/Run4_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.445  |  0.044  |  0.294  |\n",
    "| Spectralis |  0.640  |  0.717  |  0.443  |\n",
    "| Topcon     |  0.474  |  0.493  |  0.466  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.516  |  0.401  |  0.374  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 5*\n",
    "- Repetion of Run 1 without dropping patches  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run5\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=False,\n",
    "    patch_dropping=False,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "When compared with other runs, it is seen that the validation error becomes smaller than when runs use patch dropping. That happens because 50% of the loss used to train this network corresponds to the Dice loss in the background class. Since no patch dropping happens, the network gets better at segmenting the slices that do not present fluid, thus improving the loss values. However, this improved loss does not transmit to an improvement in segmentation performance, since values obtained when testing the network do not present an improvement in any metric when compared to those that do not present patch dropping, as in *Run1*.\n",
    "\n",
    "![Training Error in Run5](./imgs/Run5_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run5_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run5_slice_dice.csv), [vendor](results/Run5_vendor_dice.csv), [volume](results/Run5_volume_dice.csv), and [class](results/Run5_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.350  |  0.201  |  0.323  |\n",
    "| Spectralis |  0.675  |  0.716  |  0.423  |\n",
    "| Topcon     |  0.371  |  0.290  |  0.518  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.460  |  0.471  |  0.385  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run5_slice_dice.csv), [vendor](results/Run5_vendor_dice.csv), [volume](results/Run5_volume_dice.csv), and [class](results/Run5_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.350  |  0.201  |  0.323  |\n",
    "| Spectralis |  0.675  |  0.716  |  0.423  |\n",
    "| Topcon     |  0.371  |  0.290  |  0.518  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.460  |  0.471  |  0.385  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 6*\n",
    "- Repetion of Run 1, dropping 50% of the non-pathological patches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run6\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.50,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Similarly to the last runs, a decreased loss value is obtained in models that handle more images with no pathology/no fluid to segment.\n",
    "\n",
    "![Training Error in Run6](./imgs/Run6_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run6_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run6_slice_dice.csv), [vendor](results/Run6_vendor_dice.csv), [volume](results/Run6_volume_dice.csv), and [class](results/Run6_class_dice.csv).\n",
    "\n",
    "The results obtained in this run present a slightly worse version than those obtained in Run7, not surpassing or obtaining any significantly improved results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 7*\n",
    "- Repetion of Run 1, dropping 25% of the non-pathological patches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run7\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.25,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The results in training achieved a better loss due to a higher quantity of non-pathological scans when compared to the previous models. \n",
    "\n",
    "![Training Error in Run7](./imgs/Run7_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run7_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run6_slice_dice.csv), [vendor](results/Run6_vendor_dice.csv), [volume](results/Run6_volume_dice.csv), and [class](results/Run6_class_dice.csv).\n",
    "\n",
    "So far, Run7 showed the best Dice performance, as it achieved some of the best results among the runs, while also keeping up with the best runs across the rest, therefore showcasing the most consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 8*\n",
    "- Repetion of Run 1, dropping 100% of the non-pathological patches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run8\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=1.,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(run_name=\"Run8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The results in training achieved a significantly higher loss due to a higher quantity of non-pathological scans when compared to the previous models. \n",
    "\n",
    "![Training Error in Run8](./imgs/Run8_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run8_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. In each slice, the number of positive voxels was recorded to allow the weighted average to calculate the real Dice coefficient of each volume or vendor. Then, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the CSV files in the folder results, organized per [slice](results/Run8_slice_dice.csv), [vendor](results/Run8_vendor_dice.csv), [volume](results/Run8_volume_dice.csv), and [class](results/Run8_class_dice.csv).\n",
    "\n",
    "So far, Run8 achieved outstanding performances on Cirrus when segmenting IRF and PED and good performances on Topcon in IRF and SRF segmentation, but on the other classes was not so good. However, the performance in Spectralis was worse than in the remaining models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall Testing Results**\n",
    "\n",
    "**Runs**\n",
    "- *Run 1*: 100 Epochs, assyncronous patch extraction, and 75% drop percentage\n",
    "- *Run 2*: 200 Epochs, assyncronous patch extraction, and 75% drop percentage\n",
    "- *Run 3*: 100 Epochs, syncronous patch extraction, and 0% drop percentage\n",
    "- *Run 4*: 100 Epochs, syncronous patch extraction, and 75% drop percentage\n",
    "- *Run 5*: 100 Epochs, assyncronous patch extraction, and 0% drop percentage\n",
    "- *Run 6*: 100 Epochs, assyncronous patch extraction, and 50% drop percentage (REPEAT)\n",
    "- *Run 7*: 100 Epochs, assyncronous patch extraction, and 25% drop percentage\n",
    "- *Run 8*: 100 Epochs, assyncronous patch extraction, and 100% drop percentage\n",
    "\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "|          |              |     **IRF**    |              |             |    **SRF**     |              |             |    **PED**     |              |\n",
    "| :------: | :----------: | :------------: | :----------: | :---------: | :------------: | :----------: | :---------: | :------------: | :----------: |\n",
    "| **Run**  |  **Cirrus**  | **Spectralis** |  **Topcon**  |  **Cirrus** | **Spectralis** |  **Topcon**  |  **Cirrus** | **Spectralis** |  **Topcon**  |\n",
    "|   Run1   |    0.235     |     0.515      |    0.401     |  **0.176**  |     0.706      |  **0.100**   |    0.191    |     0.470      |    0.123     |\n",
    "|   Run2   |    0.156     |     0.463      |    0.462     |    0.106    |     0.660      |    0.014     |    0.177    |     0.411      |    0.139     |\n",
    "|   Run3   |    0.230     |     0.582      |    0.371     |    0.103    |   **0.751**    |    0.004     |    0.217    |     0.491      |    0.226     |\n",
    "|   Run4   |    0.191     |     0.513      |  **0.530**   |    0.105    |     0.690      |    0.075     |    0.193    |     0.436      |  **0.233**   |\n",
    "|   Run5   |    0.133     |   **0.592**    |    0.274     |    0.069    |     0.703      |    0.014     |    0.183    |     0.428      |    0.229     |\n",
    "|   Run6   |     NaN      |      NaN       |     NaN      |     NaN     |      NaN       |     NaN      |     NaN     |      NaN       |     NaN      |\n",
    "|   Run7   |    0.312     |     0.559      |    0.295     |    0.045    |     0.728      |    0.038     |  **0.240**  |   **0.518**    |    0.197     |\n",
    "|   Run8   |  **0.354**   |     0.500      |    0.431     |    0.059    |     0.670      |    0.039     |    0.177    |     0.319      |    0.052     |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    " Run  |   IRF   |   SRF   |   PED   |\n",
    " :--: | :-----: | :-----: | :-----: |\n",
    " Run1 |  0.355  |**0.371**|  0.223  |\n",
    " Run2 |  0.272  |  0.218  |  0.215  |\n",
    " Run3 |  0.363  |  0.275  |  0.306  |\n",
    " Run4 |  0.343  |  0.191  |  0.278  |\n",
    " Run5 |  0.259  |  0.189  |  0.251  |\n",
    " Run6 |   NaN   |   NaN   |   NaN   |\n",
    " Run7 |  0.399  |  0.190  |**0.308**|\n",
    " Run8 |**0.427**|  0.171  |  0.142  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Network\n",
      "\t1 input channels\n",
      "\t4 output channels (classes)\n",
      "\n",
      "INFO: Starting training:\n",
      "        Epochs:          100\n",
      "        Batch size:      32\n",
      "        Learning rate:   2e-05\n",
      "        Device:          cuda\n",
      "        Mixed Precision: True\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting patches\n",
      "Extracting Training Patches\n"
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run9\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=1.,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train-Test Split for the Intermediate Slice Generation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.folds_split import k_fold_split_generation\n",
    "\n",
    "k = 5\n",
    "\n",
    "if not (isfile(path=\"splits/generation_train_splits.csv\") or isfile(path=\"splits/generation_test_splits.csv\")):\n",
    "    k_fold_split_generation(k=k, folders_path=RETOUCH_PATH)\n",
    "else:\n",
    "    print(\"Split already exists. To create a new one please delete the old files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCT Volumes Reading and Saving for Generation Task\n",
    "\n",
    "Note: To make the generation task independent from the segmentation task, the images used for segmentation are being saved again in a different folder. For memory concerns, please adjust the code to reuse those saved previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init.read_oct import save_generation_oct_as_tiff\n",
    "\n",
    "if not ((exists(IMAGES_PATH + \"\\\\OCT_images\\\\generation\\\\int32\")) and (exists(IMAGES_PATH + \"\\\\OCT_images\\\\generation\\\\uint8\"))):\n",
    "    save_generation_oct_as_tiff(oct_folder=RETOUCH_PATH, save_folder=IMAGES_PATH)\n",
    "else:\n",
    "    print(\"Images have already been extracted. To extract them again, please delete the folder with the images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
