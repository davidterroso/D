\chapter{Materials and Methods}\label{Methods}
This chapter starts with an overview of the datasets selected for the fluid segmentation and intermediate slice synthesis tasks, along with the requirements needed for the training of each model and the reasoning behind the selection. 

\section{Datasets}
The training of a model for the segmentation of different types of fluid in OCT volumes requires a large number of images annotated with those types of fluids. However, the manual segmentation of large amounts of B-scans is a laborious process, which results in a shortage of publicly available annotated OCT datasets. Furthermore, the majority of these datasets contain a limited quantity of images.

\subsection{RETOUCH Dataset}

The dataset selected to be used in this dissertation was the RETOUCH dataset \parencite{Bogunovic2019b}. It consists of 112 OCT volumes, obtained with four different devices: 38 from the Cirrus HD-OCT (Zeiss Meditec), 38 from the Spectralis (Heidelberg Engineering), and 36 from the \mbox{T-1000}/T-2000 (Topcon). The 112 volumes are split into a training (70 volumes) and a test set (42 volumes). Only the OCT volumes in the training set have annotations of the retinal fluids (IRF, SRF, and PED). The annotations present in this dataset were performed by two clinicians and both segmentations were considered correct. In this dissertation, only the annotated volumes were used for training and testing the fluid segmentation.
\par
From those 70 OCT volumes, 24 were obtained with the Cirrus, 24 volumes were acquired with the Spectralis, and 22 were obtained with the two Topcon devices. The number of B-scans per volume, the dimensions of the B-scans, and the axial resolutions vary according to the device. Figure \ref{fig:CirrusTopconSpectralisRetinalLayerComparison} illustrates how different axial resolutions affect the visualization of the retinal layers. For example, each voxel in the Cirrus volumes has a height of 1.95 $\mu$m, while each voxel in the Spectralis volumes has an height of 3.87 $\mu$m. These differences in height cause the same structures to appear larger in Cirrus B-scans, as seen in this figure.
\par
The volumes acquired using the Cirrus have 128 B-scans, while those obtained with Spectralis have 49 B-scans. The volumes acquired using Topcon devices (\mbox{T-1000} or \mbox{T-2000}) typically contain 128 B-scans; however, two volumes include only 64 \mbox{B-scans}. In total, 6838 B-scans were used on the training and testing of the segmentation models.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.80\linewidth]{figures/CirrusTopconSpectralisRetinalLayerComparison.png}
	\caption{Retinal layers B-scans from different patients, using Cirrus (left), Topcon (middle), and Spectralis (right) devices. Despite representing the same structure, the Cirrus scan, the retinal layers appear much thicker than in the scans obtained with other vendors.}
	\label{fig:CirrusTopconSpectralisRetinalLayerComparison}
\end{figure}

When compared with other renowned OCT datasets annotated with retinal fluid, such as the Duke dataset \parencite{Chiu2015}, the two datasets from the University of Minnesota \parencite{Rashno2017, Rashno2018}, and the \textcite{Lu2019} dataset, the RETOUCH presents a significantly larger quantity of annotated volumes. It also offers greater variety, as the volumes were obtained using four different devices, unlike the other datasets, which include scans from only one device. In Table \ref{tab:DatasetsSummary}, a comparison between the number of annotated B-scans in each of the mentioned datasets is shown, as well as the devices utilized to obtain the OCT images, the diseases of the patients, and the distribution of annotated B-scans per OCT volume.

\begin{table*}[!ht]
	\caption{Summary of the number of OCT volumes, B-scans per volume, total number of B-scans, and associated macular diseases in each dataset.}
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		& & & & & \\ [-1.5ex] % Used to center the text vertically
		& \textbf{DUKE2015 \parencite{Chiu2015}} & \textbf{UMN2017 \parencite{Rashno2017}} & \textbf{UMN2018 \parencite{Rashno2018}} & \textbf{LU2019 \parencite{Lu2019}} & \textbf{RETOUCH \parencite{Bogunovic2019b}} \\ [1ex]
		\hline
		& & & & & \\ [-2.0ex]
		\textbf{Volumes} & 10 & 24 & 29 & 528 & 70$^{a}$ \\
		& & & & & \\ [-2.5ex]
		\textbf{B-scans/Volume} & 11 & 25 & 25 & Variable & 128 (Cirrus and Topcon$^{b}$), 64 (Topcon$^{b}$), 49 (Spectralis) \\
		& & & & & \\ [-2.5ex]
		\textbf{B-scans} & 110 & 600 & 725 & 750 & 6838 \\
		& & & & & \\ [-2.5ex]
		\textbf{Device} & Spectralis & Spectralis & Spectralis & Spectralis & Cirrus, Topcon, and Spectralis \\
		& & & & & \\ [-2.5ex]
		\textbf{Disease} & DME & AMD & DME & DME & AMD and RVO \\
		\hline
		\multicolumn{4}{l}{}
	\end{tabular}}
	\label{tab:DatasetsSummary}
	\par
	\justifying
	\footnotesize{$^{a}$ 24 volumes from Cirrus, 22 volumes from Topcon, and 24 volumes from Spectralis.}
	\par 
	\justifying
	\footnotesize{$^{b}$ Two of the training volumes obtained using the Topcon devices have only 64 slices.}
\end{table*}

The diversity and dimension of the RETOUCH dataset make it one of the most widely used datasets for the development of fluid segmentation models \parencite{Rahil2023, Zhang2023, Xing2022, Tang2022, Liu2024, Li2023, Hassan2021b, Lu2019}. These aspects also motivated the selection of the RETOUCH dataset for the implementation of fluid segmentation models in this dissertation.
\par
Regarding intermediate slice synthesis, the 112 OCT volumes that constitute the RETOUCH dataset were used for the training and evaluation of the models. The volumes that do not have segmentation masks can also be included since these masks are not necessary in the intermediate slice generation task.
\par
The consistent number of slices per volume and large quantity of OCT volumes make the RETOUCH dataset suitable for the training and evaluation of the models developed to generate intermediate slices.

\subsection{Centro Hospitalar Universitário de São João OCT Dataset}

A private dataset provided by the Centro Hospitalar Universitário de São João (CHUSJ) was also used for external evaluation of the proposed segmentation models. This dataset comprises six Spectralis OCT volumes, containing 19 slices each. The volumes vary in resolution, dimensions, noise levels, and contrast, offering diverse testing scenarios. All volumes include at least one type of fluid, though the overall fluid presence is limited. Unlike RETOUCH, which includes the annotations from two clinicians, this dataset was segmented by a single one.
\par
Despite the scans in this dataset also being obtained with a Spectralis device, the images have different dimensions. In RETOUCH, all the Spectralis scans present a size of 496 $\times$ 512 pixels (height $\times$ width; this notation is used for images shapes throughout this dissertation), while in CHUSJ dataset the images differ in width, depending on the OCT volume. The dimensions of the OCT B-scans vary between 496 $\times$ 1024, 496 $\times$ 768, and 496 $\times$ 512 pixels. Since the images in this dataset do not contain metadata specifying the resolution of each slice, the slices had to be observed individually in order to determine whether the pixels in each B-scan corresponded to the same physical dimensions. Figure 3.1 shows three OCT B-scans with different dimensions, highlighting the variation in resolution across the images.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/CHUSJDifferentResolutions.png}
	\caption{Three B-scans with different dimensions (496 $\times$ 1024, 496 $\times$ 768, and 496 $\times$ 512 pixels, respectively) from the CHUSJ dataset. As indicated by scale bars in the bottom left corners of each B-scan, the resolution also differs across slices.}
	\label{fig:CHUSJDifferentResolutions}
\end{figure}

In the same figure, it is also possible to see different image characteristics. For example, in the left image, the choroid is very well-defined, while in the middle image it looks more blurry. The noise levels also differ depending on the image. The background in the left image exhibits minimal noise, and the retinal layers are easily identifiable. In the middle image, some noise appears in the background and the retinal layers become harder to differentiate. Lastly, the image on the right contains noticeable background noise, while the retinal layers appear brighter than in the other images. 

\section{Methodology for Fluid Segmentation}
The initial experiments in this dissertation focused on training networks for the fluid segmentation task. The goal of these experiments was to determine which segmentation network performs the best in the considered task, which were later required for the fluid volume estimation.
\par
For its influence and significance for fluid segmentation papers, the U-Net \parencite{Ronneberger2015} was selected to perform segmentation of the fluid regions in each B-scan. The U-Net is distinguished by its encoder-decoder structure, which resembles the letter U (see Figure \ref{fig:UNet}). In the encoder path, two 3x3 unpadded convolutions are applied to the input image, each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with a stride of 2, which downsamples the image. At each downsampling step, the number of channels is doubled. In the expanding path, a 2x2 up-convolution is applied, halving the number of channels. The result is then concatenated with the cropped feature map from the corresponding contracting path block. A 1x1 convolution is applied in the final layer.
\par
The number of output channels in the U-Net can be adapted to the number of classes the network is segmenting. For example, if the model aims to segment the three types of retinal fluid, the number of output channels is set to 4 (background, IRF, SRF, and PED). However, in case the model is trained only for the segmentation of a single fluid, the number of output channels is then 2 (background and selected fluid).

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/UNet}
	\caption{U-Net architecture \parencite{Ronneberger2015}.}
	\label{fig:UNet}
\end{figure}

The evaluation of all segmentation networks was conducted using the Dice coefficient. The Dice coefficient is a commonly used metric for evaluating the similarity between two sets. In this context, it was used for assessing the similarity between the segmentation mask generated by the segmentation network and the GT. The formula for the Dice coefficient is shown in Equation \ref{eq:DiceCoefficient}, where $A$ is a set that represents the GT binary mask of one fluid and $B$ is another set that represents the predicted binary mask of the same fluid \parencite{Shamir2019}. 
\par
Considering the multi-class segmentation scenario, the Dice coefficient can be computed for each class independently as shown in Equation \ref{eq:DiceCoefficientPixels}, by treating the corresponding class labels as binary masks, where $a_{i}$ and $b_{i}$ represent the predicted and GT binary values for a specific class $i$. The network that achieved the highest Dice score was selected for use in the fluid volume estimation experiments.

\begin{equation}
	\text{Dice}(A, B) = \frac{2|A \cap B|}{|A| + |B|}
	\label{eq:DiceCoefficient}
\end{equation}

\begin{equation}
	\text{Dice}(A, B) = \frac{2\sum_{i} a_{i} b_{i}}{\sum_{i} a_{i} + \sum_{i} b_{i}}
	\label{eq:DiceCoefficientPixels}
\end{equation}

The loss function that regularized the training in the fluid segmentation experiments was the same as the one used by \textcite{Tennakoon2018}. This loss is described in Equation \ref{eq:SegmentationLoss}, where $\lambda_{D}$ is the weight of the Dice component, $\mathcal{L}_{D}$, and $\lambda_{CE}$ is the weight of the cross-entropy component, $\mathcal{L}_{CE}$, with both weights set to 0.5.

\begin{equation}
	\mathcal{L} = \lambda_{D} \mathcal{L}_{D} + \lambda_{CE} \mathcal{L}_{CE}
	\label{eq:SegmentationLoss}
\end{equation}

The component $\mathcal{L}_{D}$ is the Dice loss for the foreground. This reflects how well the model is at detecting and segmenting the fluid present in the B-scans. For any image, where each pixel is associated with an index $i$, the loss is described in Equation \ref{eq:SegmentationDice}, where $s_{i\overline{0}}$ is a binary variable that is 0 when the pixel $i$ belongs to the class 0 (background) and is 1 when it belongs to any class other than 0 (foreground). $p_{i\overline{0}}$ corresponds to the predicted probability of the pixel $i$ belonging to the foreground. The $\epsilon$ constant is a small value utilized to prevent division by zero.

\begin{equation}
	\mathcal{L}_{D} = 1 - \left( \frac{2 \sum_{i} s_{i\overline{0}} p_{i\overline{0}}}{\sum_{i} s_{i\overline{0}} + \sum_{i} p_{i\overline{0}} + \epsilon} \right)
	\label{eq:SegmentationDice}
\end{equation}

However, this loss component alone is not enough to correctly label the pixels into their respective classes; for this reason, the cross-entropy component was used. Due to the large class imbalance in the images, with the background occupying the majority of each image, the cross-entropy loss is balanced by taking into account the number of pixels belonging to each class. The cross-entropy is calculated for each pixel $i$ in the image. Then, for each class, the cross-entropy of all pixels belonging to that class is summed and divided by the number of pixels in the class. The mean of the values obtained for each class result finally in $\mathcal{L}_{CE}$, as can be seen in Equation \ref{eq:SegmentationCE}. In this equation, $N=4$ and represents the number of classes, while $C$ is the set of possible classes, $\{0,1,2,3\}$, which corresponds, respectively, to background, IRF, SRF, and PED.

\begin{equation}
	\mathcal{L}_{CE} = - \sum_{c \in C} \frac{1}{N}\left( \frac{1}{\sum_{i} s_{i,c}} \sum_{i} s_{i,c} \text{ln} p_{i,c} \right)
	\label{eq:SegmentationCE}
\end{equation}

Since the U-Net is a fully convolutional network, it is not dependent on the shape of the input to perform segmentation and can be trained on patches of multiple sizes. The model was initially trained on patches of size 256 $\times$ 128 pixels, following the same implementation as that in \textcite{Tennakoon2018}. The extraction of patches aimed to prioritize the B-scan information relevant for the segmentation. To achieve this, the patches are not distributed uniformly. Instead, 10 patches are extracted from a random location inside the region of interest (ROI) of each image. The image's ROI is the part of the image where the entropy is above a defined threshold or where retinal fluid is present. Of the patches with no fluid, 75\% were dropped. In Figure \ref{fig:FluidAndROI}, it is possible to see the overlaying of the fluid masks and the ROI, with a red bounding box signaling a patch that would be used as input to train the U-Net.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/FluidAndROI.png}
	\caption{Cirrus B-scan (left), fluid masks overlay (middle) with IRF in red, SRF in green, and PED in blue, and the ROI mask overlaid in purple (right). The red bounding box signals a possible 256 $\times$ 128 patch that could be extracted.}
	\label{fig:FluidAndROI}
\end{figure}

The second patch size experimented was significantly larger. They were no longer randomly extracted from the ROI. Instead, the patches were extracted from top to bottom so that every section of the image was present in at least one patch. A representation of this process can be seen in Figure \ref{fig:BigPatchExtraction}. The patches were extracted this way with the intent of including the entire retina and fluid regions within a single patch, in order to preserve spatial coherence and avoid degrading input quality.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/BigPatchExtraction.png}
	\caption{Cirrus B-scan with its three corresponding patches of size 496 $\times$ 512.}
	\label{fig:BigPatchExtraction}
\end{figure}

In the third patch extraction method tested, all the B-scans images were resized to the same size. As seen in Figure \ref{fig:CirrusTopconSpectralisRetinalLayerComparison}, the images present different appearances across the vendors, since the real dimensions of each voxel are different, depending on the device used to obtain the volume. These differences across vendors makes the learning of the segmentation harder. Therefore, by resizing all the images to the same shape, the structures would present more consistent dimensions across vendors and the voxels would roughly translate to the same dimensions, leading to an easier learning process for the model.
\par
After image resizing, vertical patches of dimension 496 $\times$ 128 pixels, were extracted from each resized B-scan. The number of patches extracted from each image was changed, experimenting with four (Figure \ref{fig:CirrusFourPatchExtraction}), seven (Figure \ref{fig:CirrusSevenPatchExtraction}), and thirteen (Figure \ref{fig:CirrusThirteenPatchExtraction}) patches. To extract seven and thirteen patches, they were sampled at regular intervals along the horizontal axis, starting from the left of the image and ending at the last position where a full-width patch could be extracted.
\par
The advantage of extracting vertical patches is that each image contains both the complete retinal layer and the background. This does not happen in the previous experiments, where the patches are either too small to contain both background and the retinal layers (in the first patch shape) or the retinal layers are cropped during patch extraction (in the second patch shape, as seen in Figure \ref{fig:BigPatchExtraction}).

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/CirrusFourPatchExtraction.png}
	\caption{Four vertical patches of dimension 496 $\times$ 128 extracted from a Cirrus B-scan.}
	\label{fig:CirrusFourPatchExtraction}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/CirrusSevenPatchExtraction.png}
	\caption{Seven vertical patches of dimension 496 $\times$ 128 extracted from a Cirrus B-scan.}
	\label{fig:CirrusSevenPatchExtraction}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/CirrusThirteenPatchExtraction.png}
	\caption{Thirteen vertical patches of size 496 $\times$ 128 extracted from a Cirrus B-scan.}
	\label{fig:CirrusThirteenPatchExtraction}
\end{figure}

In the first segmentation experiment, a single U-Net was utilized to perform multi-class segmentation, outputting a four channel mask. However, in the second experiment, three binary U-Nets were used in the segmentation of all fluids, with one network being trained for the segmentation of each fluid.
\par
One of the main problems with multi-class segmentation performed by separate binary models is the merging of the multiple masks. In this context, multiple fluid classes can be predicted for the same pixel, although only one of those can be correct. In this experiment, two alternatives were explored: 

\begin{itemize}
	\item order of priority, where the merging of the fluid masks follows a predefined hierarchy that determines which fluid takes precedence
	\item highest probability, where the assigned class is the one predicted with the highest probability by its model.
\end{itemize} 

Two different losses were used for training the models. Initially, the same loss as the one used in multi-class experiment was applied, using only two classes (background and the fluid that would be segmented). Later, a weighted cross-entropy loss was used, with weights balancing the larger proportion of background voxels. This second loss is described in Equation \ref{eq:SegmentationCE}, with $N=2$.
\par
After selecting the best segmentation model, the inference in CHUSJ required the support of a retinal layer segmentation model, developed by \textcite{Melo2023}, to ensure anatomically consistent predictions. This necessity comes from the differences in image quality between the CHUSJ and the RETOUCH dataset, on which the model was trained. For instance, in the RETOUCH, the Spectralis B-scans present the choroid region blurred, while in the CHUSJ, this region appears with much greater quality and definition. This discrepancy confuses the segmentation model, which often misinterprets the well-defined choroid region as IRF, leading to incorrect the segmentations.
\par
The retinal layer segmentation model was used exclusively to post-process the masks predicted by the segmentation model, removing any fluid that appeared outside the retina, as a way to mitigate the oversegmentations that occurred in the choroid region.

\section{Methodology for Intermediate Slice Synthesis}
The objective of the experiments in intermediate slice synthesis was to improve the inter-slice resolution of OCT scans in order to obtain more accurate fluid volume estimates.
\par
The intermediate slices were produced using a generative model also trained on the RETOUCH dataset. In this experiment, subvolumes consisting of overlapping triplets of consecutive slices, sampled with a step size of 1, were used, as shown in Figure \ref{fig:FrameInterpolationFramework}. The first and last slices of these triplets were used for the generation of the middle slice. Therefore, the quality of the generated slice can be evaluated by comparing it with to the original one, as done in similar studies in the literature. For each volume, the number of potential subsets is then determined to be $n-2$, where $n$ represents the number of slices within that volume.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/FrameInterpolationFramework.png}
	\caption{Scheme explaining the input data of the generative models. Each frame refers to a B-scan from an OCT volume. Extracted from \textcite{Tran2020}.}
	\label{fig:FrameInterpolationFramework}
\end{figure}

The generation of slices can be evaluated using specific metrics, as well as through qualitative assessment. To assess the performance of the generative model, the fluid segmentation model could also be used for the estimation of the fluid area in the generated image, and the resulting mask could then be compared to the original image's fluid mask. This comparison could be conducted using the Dice coefficient \parencite{Lopez2023} (see Equation \ref{eq:DiceCoefficientPixels}). However, this metric is insufficient for fully evaluating the generative model performance, as it only compares the segmented fluid region and does not assess the entire slice. Examples of other metrics include the mean absolute error (MAE) \parencite{Lopez2023, Wu2022, Zhang2022}, the peak signal-to-noise ratio (PSNR) \parencite{Xia2021, YChen2018, Sanchez2018, Fang2022, Nimitha2024, Kudo2019, You2020, Zhang2024, Zhang2022}, and the structural similarity index measure (SSIM) \parencite{YChen2018, Sanchez2018, Fang2022, Nimitha2024, Kudo2019, You2020, Zhang2024, Zhang2022}.
\par
The MAE and mean squared error (MSE) quantify the differences between the original image and the generated image. For every pixel, the difference between the value in the original image and the generated image is calculated. In MAE, the absolute value of this difference is computed, and then the mean over all pixels in the image is obtained. In contrast, in MSE, the difference is squared before computing the mean. Equation \ref{eq:MAEEquation} and Equation \ref{eq:MSEEquation} describe MAE and MSE, respectively, where $x_{i}$ is the intensity of the pixel of index $i$ in the predicted image, $y_{i}$ is the intensity of the pixel with index $i$ in the original image, and $N$ is the number of pixels in the image \parencite{Sara2019, Rajkumar2016}.

\begin{equation}
	\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |x_i - y_i|
	\label{eq:MAEEquation}
\end{equation}

\begin{equation}
	\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} \left( x_i - y_i \right)^{2}
	\label{eq:MSEEquation}
\end{equation}

PSNR is a metric used for computing the ratio between the maximum signal power (which corresponds to the maximum value of a pixel in the image) and the power of the distorting noise, which affects the quality of its representation. Therefore, the PSNR, described in Equation \ref{eq:PSNREquation}, is inversely proportional to the mean squared error, where $L$ is the pixel's maximum intensity in the image. It can also be interpreted as a representation of absolute error in decibels \parencite{Sara2019}. It is important to note that in OCT imaging the signal-to-noise ratio is relatively low due to the presence of speckle in the images. Therefore, lower PSNR values are expected, compared to other imaging techniques that do not exhibit such noise \parencite{Bogunovic2019a}.

\begin{equation}
	\text{PSNR} = 10 \cdot \log_{10} \left( \frac{L^2}{\text{MSE}} \right)
	\label{eq:PSNREquation}
\end{equation}

While the previous metrics focus on the differences between two images at a pixel level, SSIM is based on the perceptual quality of the image. This metric considers changes in perceived structural information, estimating the visual quality of images and videos. SSIM measures the similarity between the original image and the generated one, and it is calculated as shown in Equation \ref{eq:SSIMEquation}. Here, $x$ and $y$ represent the generated and original images, respectively. Here, $\mu_{x}$ and $\mu_{y}$ are their local means, $\sigma_{x}$ and $\sigma_{y}$ are their standard deviations, and $C_{1}$ and $C_{2}$ are small constants that stabilize the division. The contrast sensitivity (CS) between the images $x$ and $y$ is represented by $CS(x,y)$ \parencite{Sara2019}.

\begin{equation}
	\text{SSIM}(x, y) = \left( \frac{2\mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \right) \cdot \left( \frac{2\sigma_{xy} + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \right) = \left( \frac{2\mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \right) \cdot CS(x, y)
	\label{eq:SSIMEquation} 
\end{equation}

Since the best-performing models in segmentation resized the images to 496 $\times$ 512, the images were generated to match those dimensions. Therefore, regardless of the device used for acquiring the OCT volume, all B-scans were resized to 496 $\times$ 512 for both image generation.
\par
In the first experiment focused on intermediate slice synthesis, a GAN was used. The underlying principle of a GAN, originally proposed by \textcite{Goodfellow2014}, is based on a competitive game between two networks. In a scenario where a GAN is employed for inter-slice SR, the generator network takes the first and last slices of a subvolume, composed of three consecutive B-scans from an OCT scan, and aims to generate the intermediate slice. In contrast, the discriminator network is trained to distinguish between the generated and real slices. When the discriminator correctly labels generated slices as fake, the generator is penalized, motivating it to fool the discriminator and, consequently, improve its output, resulting in slices more similar to the real ones. However, the discriminator network loss also penalizes misclassifications, depending on the probability of the prediction. As a result, as the generator improves, so does the discriminator \parencite{Goodfellow2020}. The overall framework for GANs is illustrated in Figure \ref{fig:GANFramework}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/GANFramework}
	\caption{Example of a GAN framework, where $\mathcal{D}$ is the discriminator and $\mathcal{G}$ is the generator \parencite{Creswell2018}.}
	\label{fig:GANFramework}
\end{figure}

The GAN originally developed by \textcite{Tran2020} for interpolating intermediate frames in video was adapted in this work to generate the intermediate slices in OCT volumes. The network training was performed on image patches of 64 $\times$ 64 pixels.
\par
The generator of the GAN is composed of a contracting and an expanding path. In the contracting path, convolutions are applied to the images and feature maps, followed by batch normalization, and a leaky ReLU activation function. After the images are downsampled to 512 $\times$ 8 $\times$ 8, reaching the bottleneck, the expanding path begins, where deconvolutions are applied, followed by batch normalization and a leaky ReLU. Finally, after the last deconvolution, the hyperbolic tangent function is used as the final activation function, resulting in an output of size 64 $\times$ 64, in the range of -1 to 1. An illustrative scheme of the generator is shown in Figure \ref{fig:GeneratorArchitecture}.

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{figures/GeneratorArchitecture}
	\caption{Architecture of the generator used in the GAN. It has a contracting and an expanding path, making it a U-Net like network \parencite{Tran2020}.}
	\label{fig:GeneratorArchitecture}
\end{figure}

In the original implementation, one patch is randomly extracted from the triplet of images and used in training. Due to the much smaller amount of data available in OCT, all possible disjoint 64 $\times$ 64 patches were extracted from each B-scan. The extraction was performed from top to bottom, and in the last row of slices, the image was padded with zeros until it reached 64 pixels. An example of the patches extracted from a Cirrus B-scan is shown in Figure \ref{fig:CirrusSixtyFourPatchExtraction}. By methodically extracting the patches, triplets could be easily formed by accessing patches at the same index in the three consecutive images.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/CirrusSixtyFourPatchExtraction.png}
	\caption{Patches of size 64 $\times$ 64 extracted from a Cirrus B-scan previously resized to 496 $\times$ 512.}
	\label{fig:CirrusSixtyFourPatchExtraction}
\end{figure}

To match the needs of our application, some changes had to be made in the generator regarding the input size. As shown in Figure \ref{fig:GeneratorArchitecture}, the input had six channels - one red, one green, and one blue (RGB) for each input patch. Similarly, the output had three channels, for the generated middle patch. In our application, each input patch has one channel, since the OCT B-scans are grayscale images, rather than RGB, as in the original implementation. Therefore, the output has one channel. The final activation function, the hyperbolic tangent, outputs values in the range of -1 and 1. After applying the hyperbolic tangent, the results had to be converted to the 0 to 1 range, making them comparable to the GT images.
\par
The GAN's discriminator receives a 64 $\times$ 64 patch as input and outputs a probability of the input patch being real. The discriminator is composed of five consecutive convolutional layers. The first convolution is followed by a leaky ReLU activation function. The second, third, and fourth convolutions are followed by batch normalization and a leaky ReLU activation function. After the last convolution, the sigmoid is applied and converts the final output to a value between 0 and 1 that represents the probability of the image being real. In Table \ref{tab:GeneratorDiscriminatorArchitecture}, the layers that compose the discriminator and the generator are explained, including the dimensions of the outputs.

\begin{table*}[!ht]
	\setlength{\tabcolsep}{6pt}
	\renewcommand{\arraystretch}{1.3}
	\caption{Layers that compose the generator and the discriminator. Each convolution is represented by Conv2d(K, OC, S), where K is the kernel size, OC is the number of output channels, and S is the stride. The same notation is used in deconvolutions, represented by TransposedConv2d. The output size is shown following C $\times$ H $\times$ W notation, where C is the number of channels, H is the height, and W is the width. The inputs have a size of $1 \times 64 \times 64$. Adapted from \textcite{Tran2020}.}
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{c c c}
		\hline
		\hline
		\multicolumn{3}{c}{\textbf{Generator}} \\
		\hline
		\textbf{Layers} & \textbf{Details} & \textbf{Output Size (C $\times$ H $\times$ W)} \\
		\hline
		\textbf{1} & Conv2d(3, 128, 1), BatchNorm2d, LeakyReLU & 128 $\times$ 64 $\times$ 64 \\
		\textbf{2} & Conv2d(4, 128, 2), BatchNorm2d, LeakyReLU & 128 $\times$ 32 $\times$ 32 \\
		\textbf{3} & Conv2d(4, 256, 2), BatchNorm2d, LeakyReLU & 256 $\times$ 16 $\times$ 16 \\
		\textbf{4} & Conv2d(4, 512, 2), BatchNorm2d, LeakyReLU & 512 $\times$ 8 $\times$ 8 \\
		\textbf{5} & TransposedConv2d(4, 256, 2), BatchNorm2d, LeakyReLU & 256 $\times$ 16 $\times$ 16 \\
		\textbf{6} & TransposedConv2d(4, 128, 2), BatchNorm2d, LeakyReLU & 128 $\times$ 32 $\times$ 32 \\
		\textbf{7} & TransposedConv2d(4, 64, 2), BatchNorm2d, LeakyReLU & 64 $\times$ 64 $\times$ 64 \\
		\textbf{8} & TransposedConv2d(1, 1, 1), Tanh & 1 $\times$ 64 $\times$ 64 \\
		\hline
		\hline
		\multicolumn{3}{c}{\textbf{Discriminator}} \\
		\hline
		\textbf{Layers} & \textbf{Details} & \textbf{Output Size (C $\times$ H $\times$ W)} \\
		\hline
		\textbf{1} & Conv2d(4, 64, 2), LeakyReLU & 64 $\times$ 32 $\times$ 32 \\
		\textbf{2} & Conv2d(4, 128, 2), BatchNorm2d, LeakyReLU & 128 $\times$ 16 $\times$ 16 \\
		\textbf{3} & Conv2d(4, 256, 2), BatchNorm2d, LeakyReLU & 256 $\times$ 8 $\times$ 8 \\
		\textbf{4} & Conv2d(4, 512, 2), BatchNorm2d, LeakyReLU & 512 $\times$ 4 $\times$ 4 \\
		\textbf{5} & Conv2d(4, 1, 1), Sigmoid & 1 $\times$ 1 $\times$ 1 \\
		\hline
		\hline
	\end{tabular}}
	\label{tab:GeneratorDiscriminatorArchitecture}
\end{table*}

In the training of a GAN, both the generator and the discriminator are trained sequentially and independently. First, the generator, which receives the first and last slice patch of the input triplet, attempts to generate the intermediate patch. The generated image is then compared to the original image, using the generator loss, which is then used for updating the generator weights. The generator loss is composed of four components: the adversarial loss, the MAE, the multi-scale SSIM (MS-SSIM) loss, and the gradient difference loss (GDL). The overall loss function is described in Equation \ref{eq:GeneratorLoss}, where $\lambda_{\text{adv}}=0.05$, $\lambda_{\text{MAE}}=1.0$, $\lambda_{\text{MS-SSIM}}=6.0$, and $\lambda_{\text{GDL}}=1.0$, representing the weights of each loss component.

\begin{equation}
	\mathcal{L}_{\text{Gen}} = \lambda_{\text{adv}} \times \mathcal{L}_{\text{adv}} + \lambda_{\text{MAE}} \times \mathcal{L}_{\text{MAE}} + \lambda_{\text{MS-SSIM}} \times \mathcal{L}_{\text{MS-SSIM}} + \lambda_{\text{GDL}} \times \mathcal{L}_{\text{GDL}}
	\label{eq:GeneratorLoss}
\end{equation}

The adversarial loss evaluates of how effectively the output from the generator fools the discriminator. This evaluation is done using the binary cross-entropy (BCE). To calculate this, the generated image is passed to the discriminator, which then outputs the probability that it is real. The better the generator fools the discriminator, the closer the output of the discriminator is to one, and, therefore, the closer the adversarial loss is to 0. The adversarial loss is explained in Equation \ref{eq:AdversarialLoss}, where $\mathcal{D}$ is the discriminator, $x$ is the generated image, and $y$ is the label that indicates that the image is real. It is important to note that during the generator training, the images that are input to the discriminator are detached, not contributing to the updating of the discriminator weights in this step.

\begin{equation}
	\mathcal{L}_{\text{adv}} (\mathcal{D}(x), y) = \mathcal{L}_{\text{BCE}} (\mathcal{D}(x), y) = - \left[ y \text{log}(\mathcal{D}(x)) + (1 - y)\text{log}(1 - \mathcal{D}(x)) \right]
	\label{eq:AdversarialLoss}
\end{equation}

The MAE loss, also referred to as $L_{1}$ loss, performs a pixel-by-pixel comparison between the generated image, $x$, and the real image, $y$, as described in Equation \ref{eq:MAEEquation}, where $i$ is the index of a pixel. Although this loss gives insight into how similar the images are, on average, it can be deceiving, since the model can blur the output to attain better MAE values. For this reason, it must be combined with other reconstructive losses, such as the MS-SSIM and the GDL.
\par
The MS-SSIM loss seeks to preserve the structural similarity, at different scales, between the real and the generated images, facilitating a smoother output. This loss, originally suggested by \textcite{Wang2003}, is described in Equation \ref{eq:MSSSIMLoss}, while the MS-SSIM used in this implementation is explained in Equation \ref{eq:MSSSIM}, using the concepts of SSIM and CS introduced in Equation \ref{eq:SSIMEquation}. This version of the MS-SSIM is faster than the original implementation \parencite{Wang2003}, while using the same array of weights, $\beta$, and number of levels, $M$, which is set to 5. Therefore, the MS-SSIM corresponds to the product of the contrast sensitivity in the image for the first four levels and the SSIM of the image at the last level, all raised to the power of the respective level's weights.

\begin{equation}
	\mathcal{L}_{\text{MS-SSIM}} (x, y) = 1 - \text{MS-SSIM}(x, y)
	\label{eq:MSSSIMLoss}
\end{equation}

\begin{equation}
	\text{MS-SSIM}(x, y) = \prod_{j=1}^{M-1} \left[ \text{CS}_j(x, y) \right]^{\beta_j} \cdot \left[ \text{SSIM}_M(x, y) \right]^{\beta_M}
	\label{eq:MSSSIM}
\end{equation}

The last component of the generator loss (Equation \ref{eq:GeneratorLoss}) is the GDL, originally proposed by \textcite{Mathieu2016}. This component is used to reduce for reducing motion blur in the generated images, a problem in video datasets. In this loss, the relative difference of neighboring pixels between the generated and true images is considered, as shown in Equation \ref{eq:GDLLoss}. In this equation, $i$ and $j$ are the pixels coordinates, with $\alpha$ set to 2.

\begin{equation}
	\mathcal{L}_{\text{GDL}}(x, y) = \sum_{i,j} \left( \left|\,|x_{i,j} - x_{i-1,j}| - |y_{i,j} - y_{i-1,j}|\,\right|^{\alpha} + \left|\,|x_{i,j} - x_{i,j-1}| - |y_{i,j} - y_{i,j-1}|\,\right|^{\alpha} \right)
	\label{eq:GDLLoss}
\end{equation}

After the images are generated and evaluated using the previously defined generator loss, two images are input, sequentially, to the discriminator, with one of them being fake while the other is real. The discriminator outputs the probability of each image being real, and this is compared to the true label of each image using the BCE. The BCE is calculated for the probabilities predicted by the discriminator and the image's respective label, as described in Equation \ref{eq:AdversarialLoss}. This is done for the fake image and for the real image. The mean of the BCE values for the fake and real images is the discriminator loss.
\par
In the second intermediate slice synthesis experiment, the selected network was the U-Net, inspired by the work of \textcite{Nishimoto2024}. While the U-Net is more commonly applied in segmentation, as seen in the reviewed literature, \textcite{Nishimoto2024} applied it to generate the intermediate slices of a subvolume. The U-Net receives the edge slices as input and produces the intermediate slices as output. In their paper \parencite{Nishimoto2024}, this was tested for three, four, and five slices. However, in this experiment it was utilized to generate a single intermediate slice. The loss used to regularize the network training was the MAE, previously described in Equation \ref{eq:MAEEquation}.

\section{Methodology for Fluid Volume Estimation}
The estimation of fluid volume was done using the optimal segmentation and intermediate slice generation models. The GAN was used for generating the intermediate slices in the OCT volumes, while the best performing multi-class segmentation U-Net model generated segmentation masks for both the unaltered volumes and the volumes with generated slices. 
\par
The OCT scans used in the fluid volume estimation experiments were those from the \hbox{RETOUCH} dataset that composed the reserved fold in the segmentation. These volumes were used because no model was trained or validated on them, allowing an insight into both models generalization on unseen data and how the increase in resolution affects the total fluid volume.
\par
The area of each fluid in each OCT scan was estimated considering the resolution of each OCT scan, which varies according to the device used for obtaining the OCT volume. Afterwards, the area is multiplied by the axial distance (half the axial distance to the previous slice plus half the axial distance to the following slice) to obtain the volume of fluid per slice. In the first and last slice of an OCT volume, the area is multiplied by half of the axial distance (half the axial distance to the neighboring slice). The total volume of fluid in an OCT scan can be estimated by summing the fluid volumes attributed to all individual B-scans, which results from the fluid area in each slice. This allows the volume estimation of IRF, SRF, and PED, as well as the overall fluid volume in the OCT scan.
\par
The total volume of fluid from class $c$ in a slice of index $s$ is defined in Equation \ref{eq:FluidEstimationSlice}. The slice belongs to an OCT volume obtained using device $D$ and its total number of B-scans is defined as $S$. In this equation, $H_{D}$ and $W_{D}$ are the height and width of a voxel, respectively, obtained with device $D$, while $d_{D,s,s+1}$ is the axial distance between the slice of index $s$ and the slice of index $s+1$, a value that depends on characteristics of the device $D$. The variable $l_{i}$ is the label attributed to the voxel of index $i$. Like the variable $c$, $l$ can be one of the following classes: $\{0,1,2,3\}$, which respectively correspond to background, IRF, SRF, and PED. Meanwhile, the total volume of fluid from a class $c$ in an OCT scan obtained with device $D$ is described by Equation \ref{eq:FluidEstimationVolume}, and consists of the sum of the fluid's volume attributed to each B-scan that composes the OCT, which is obtained from the fluid area in each slice.

\begin{equation}
	\begin{aligned}
		f_{c,s,D} &= \sum_{i} \left( v_{i,s,D} \times y_{i,c} \right) \quad \text{where: }\\
		v_{i,s,D} &=
		\begin{cases}
			0.5 \times H_{D} \times W_{D} \times d_{D,s,s+1} & \text{if $s=0$}\\
			0.5 \times H_{D} \times W_{D} \times d_{D,s,s-1} & \text{if $s=S$}\\
			0.5 \times H_{D} \times W_{D} \times d_{D,s,s-1} + 0.5 \times H_{D} \times W_{D} \times d_{D,s,s+1} & 
			\text{otherwise}\\
		\end{cases}\\
		y_{i,c} &=
		\begin{cases}
			1 &\text{if } l_{i} = c\\
			0 &\text{if } l_{i} \neq c\\
		\end{cases} 
		\label{eq:FluidEstimationSlice}
	\end{aligned}
\end{equation}

\begin{equation}
	F_{c,D} = \sum_{s=0}^{S} f_{c,s,D}
	\label{eq:FluidEstimationVolume}
\end{equation}

The fluid volumes resulting from both experiments were compared. Since there is no true value for the fluid quantity in the OCT scans, the results were compared with each other. Therefore, the results would be deemed satisfactory in case they do not vary more than an order of magnitude between each other. In case a significant difference was observed, the generated images and their respective masks were analyzed, in order to understand what caused the observed difference between experiments. 
\par
Using the GAN, the corresponding fake OCT volumes of the reserved fold were created. In these volumes, the segmentation model inferred the fluid masks and they were compared to the GT masks of the original volumes, using the Dice coefficient. 
\par
Also utilizing the GAN, the OCT volumes were super-resolved, as a slice was generated between each pair of two known slices. To calculate the fluid volume in the super-resolved volumes, the OCT's inter-slice distance was reduced in half.