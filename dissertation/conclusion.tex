\chapter{Conclusion}\label{Conclusion}

Throughout this dissertation, two main tasks were tackled in retinal OCT: fluid segmentation and intermediate slice generation. Theses tasks were explored with the goal of improving the automatic retinal fluid characterization, which includes the segmentation, identification, and quantification of IRF, SRF, and PED. This final chapter revisits the key findings and insights obtained from the experiments performed, presenting the strengths and limitations of the proposed methods.
\par
In the first experiments, multiple approaches to the multi-class segmentation were studied. The segmentation experiments started with a baseline U-Net, used in the multi-class segmentation of fluids. While  still using this network, the input was changed, testing different patch shapes and extraction procedures, which provided an insight on how each affects the network performance.
\par
When using smaller patches, extracted predominantly from the retina, the U-Net failed to understand the anatomic boundaries and relationships that define the retina, limited by the area of the image captured in this input. Therefore, the model would often segment outside of the retina and predictions while different fluids would be attributed to similar regions.
\par
The use of larger patches, which occupied the majority of each the B-scan, significantly improved the model's performance, but did not solve all the problems seen in smaller patches. While the model better understood the limits imposed by the retinal layers and learned how to leverage these landmarks in segmentation, the segmentations lacked detail in smaller regions. This is a common issue in models trained with the full image or larger patches. The model learns the bigger features better, such as the relationships between layers and fluids, but fails to grasp the smaller details, such as small fluid regions, commonly merging them.
\par
During the extraction of these larger patches, the retinal layers would sometimes be divided in two independent patches, which also affected the perception of the retinal layers, leading to segmentations outside the retina. Therefore, the models trained with larger patches lacked both the capability of accurately segmenting the fluid and a flawless understanding the influence which the retinal layers have in segmentation.
\par
As a way to find a compromise between the large patches which lack the detail in their segmentations and the smaller patches which can not grasp the importance of the anatomic references, a different patch shape was used. These patches were larger vertically than horizontally, capturing the whole height of the image, but small enough along the horizontal direction so that the model can focus on the smaller details. Complementing this, all the images were also resized to same shape. This made the same structures appear similarly, independently of the OCT device used to obtain the B-scan.
\par
With these implementations, the results significantly improved when compared to the previous patch shapes. The model understood the relationships between the anatomic landmarks in the retina and the different fluid regions, while still performing detailed segmentations in complicated areas, with consistent performances across the multiple fluids.
\par
Still, new patch extraction methods were tested and instead of dividing each image in four disjoint vertical patches, seven and thirteen overlapping patches were extracted. When training the model in these two conditions, the model learned significantly faster, reaching is maximum potential earlier than when trained with four patches. Since in each epoch the number of images seen increased, the progress was faster. While this did not translate to a performance increase when training with thirteen patches, the results shown in when using seven patches were better.
\par
Different rotations used in the transformation of the inputs were tested, with the goal of mitigating wrong segmentations that were happening. In these segmentations, the fluids would appear in senseless anatomic positions. The model was tested with no rotation, $5^{\circ}$, and $10^{\circ}$ maximum rotation. The performance of the model significantly decreased without rotation, an important transformation in OCT, where volumes appear with different inclinations of the retina. With a maximum $5^{\circ}$ rotation, the performance was slightly worse than when using $10^{\circ}$, but it mitigated the wrong segmentations that were being tackled. For this reason, the best model with $5^{\circ}$ rotation was selected to perform the final inference in unseen data, which includes the reserved fold from the RETOUCH dataset and the entirety of the CHUSJ dataset.
\par
In Experiment 2, a binary segmentation U-Net was trained to segment each of the three fluids. The motivation behind this experiment was that by simplifying the task, the resulting segmentation would be better. However, this did not happen, as the classification of fluid in its different classes by a single model, motivated it to better learn the anatomy of the retina and its relation with the fluids.  As the model's task was changed from multi-class to binary segmentation, the model did not understand this relation as deeply as the multi-class model did. Instead, the model relied more on the pixel intensity to make its predictions. For these reasons, the model would constantly oversegment fluids, often outside the retina.
\par
In this Experiment, two different data splits were used to train each model. The first data split was the one used in multi-class segmentation, while the second was specifically made for each fluid desired to segment.
\par
The performances changed significantly depending on the split used to train the models. While the average Dice coefficients saw small improvements, the performances presented less deviation across folds since the data was distributed more uniformly. The models selected to segment unseen data were those which obtained the best segmentation performances and all of them were trained on fluid-specific splits.
\par
The first models were trained using the same loss as the multi-class, which was a combination of Dice and cross-entropy. However, these binary segmentation models were also implemented using just the cross-entropy. The results obtained in segmentation when using this loss were much worse, as the model kept predicting fluid in slices where it was not present.
\par
The performances in the unseen RETOUCH volumes were satisfying, as the results obtained with the multi-class model were better than the performance on validation, while those obtained with the binary models performed similarly to the validation, in general. These results highlight the models' robustness, since it had not been nor trained nor validated in this data.
\par
When comparing these two approaches with those in the literature and past works which were also trained and evaluated in RETOUCH, the results were satisfying. Among other disadvantages, the models implemented in these experiments are simpler than any of those to which it was compared. Yet, it achieves similar or comparable results with less training duration and complexity.
\par
In the CHUSJ, the performance was worse than in the RETOUCH dataset. While the segmentation of the fluid regions was decent and all the fluids were correctly identified, the different characteristics of the dataset lead to a poor overall performance. The different images' quality, which highlighted the choroid region, lead to the common segmentation of IRF in this region. Meanwhile, the different criteria in the segmentation of PED resulted in multiple instances of oversegmentation.
\par
Overall, the fluid segmentation experiments were pivotal to the understanding of how the input shape and the patch extraction influence the segmentation of fluid in OCT, particularly through the observation of the models' learning process. 
\par
The first experiment in intermediate slice generation evaluated the use of a GAN to generate a single B-scan between two consecutive real B-scans. The results from the GAN were very visually similar to the real slices, as the generated retinal layers, speckle noise, and some fluid regions were really similar to the examples with which it learned.
\par
At a pixel level, the metrics used in the comparison between images revealed significant differences. However, these values were highly influenced by the presence of speckle noise in the images, which are difficult to predict due to their random nature. Therefore, an analysis between generated images must always be complemented with a perceptual loss and observation, and visually, the images were really similar.
\par
However, the generation of these intermediate slices is really dependent on the characteristics of the inputs that input to it. For example, the intermediate slices in OCT volumes obtained using the Cirrus device were easier to generate, as the distance between two consecutive slices was significantly smaller than in Spectralis. In the scans obtained with this last vendor, the distance between slices is larger, which resulted in rougher transitions between consecutive scans. This translates to a poorer quality in the generated images, as the information is harder to predict.
\par
The larger distance between slices seen in Spectralis, is compensated by the lower noise levels present in the scans obtained with this device when compared to others. This leads to better metrics that compare the images at a pixel level, such as PSNR and SSIM, even if it does not equate to more realistic images.
\par
The characteristics of each fluid also affect the quality of these regions in generated slices. For example, the generation of SRF, which is a fluid that usually appears as an homogeneous and well defined shape that occupies a large portion of the retina, is easier to generate between slices, as their visual aspect does not change quickly in consecutive scans and the modifications that occur in this transition are easy to predict.
\par
The generation of PED regions in intermediate slices, follows a similar pattern as the SRF, as most of the PED regions are well defined and with a considerable size. Nevertheless, there are some cases of small PED where its generation is complicated, as the boundaries of the fluid tend to be more irregular.
\par
Generating IRF in an intermediate slice is the most difficult part of slice generation. This fluid usually appears in small regions with irregular borders, which are separated by thin retinal tissue structures. Between two consecutive B-scans, even in the OCT volumes obtained with the devices of highest inter-slice resolution, these structures rapidly change its shape and visual aspect. This makes it hard to predict the borders, size, and even if the regions are merged in the intermediate slice. This issue particularly affected the Spectralis volumes, as the larger inter-slice distance made the transitions between scans rougher and the prediction of intermediate slices harder.
\par
Visually, the generated IRF regions did not look natural and some were confusing, not exhibiting the characteristics that define this fluid. Nevertheless, the GAN did not improve on the generation of IRF, as the network's loss components mainly targeted the differences between generated and real images at a pixel level. While this worked in the dataset from which the GAN was adapted, it does not work as well in OCT, where the images are corrupted with high levels of speckle noise. In this conditions, the model optimizes to generate the noise which is, at an human level, almost imperceptible, while not improving as much the harder regions to generate that are actually understood by the human eye, such as the IRF regions.
\par
The results from Experiment 4 reveal the necessity of using a more complex approach to the generation task, in OCT. While the methods used in this experiment were enough to produce satisfying results in the original article, since the CT dataset used was less noisy, they are not robust enough to be applied in OCT.
\par
The capability of the U-Net to generate images is fundamentally constrained by the loss that regularizes the training. When using the MAE loss, the network can only compare the generated slices with its GT at a pixel level, without considering the general perception of the image. As a result, the U-Net produces blurry images, particularly in the regions with noise and uncertainty. In these regions, since predicting the exact noise pattern is impossible, the network learns to average the possible variations, smoothing the final output. This minimizes the MAE, at the cost of sharpness and realism.
\par
Since the speckle noise corrupts every region of the B-scans, the entirety of the output is blurred. This issue is mitigated by the addition of an adversarial loss in the GAN in Experiment 3. The adversarial noise, which corresponds to the ability to fool the discriminator network, that evaluates the perception of the image and promotes the representation of noise in the generated images, making them more similar to the real outputs. This highlights the need of a more complex and robust approach to the generation of OCT, where images are significantly affected by noise.
\par
The use of the entire image instead of the small patches, as done in Experiment 3, also contributes to the model generating images with less detail. The use of smaller patches motivates the network to focus on local structures more than the overall scan, resulting in an image richer in detail.
\par
In Experiment 5, where the volumes of each fluid in the OCT scans that compose the reserved fold were calculated, conclusions were drawn that supported the previous statements. 
\par
While the calculated volumes in the predicted masks were similar to those in the GT without presenting any extreme outlier, the model exhibits consistent oversegmentation. In most OCT volumes and particularly in IRF, the model segmented more fluid than there was in the GT. This oversegmentation is highly related with the RETOUCH dataset, in which it was trained, which promotes oversegmentation, as seen in the fluid segmentation experiments. 
\par
However, the oversegmentation in IRF is not only rooted to the dataset, but also to the model capabilities. While the shape and intensity of the SRF and PED in the training data is similar to those seen in the reserved fold as these characteristics are more consistent in these fluids, IRF presents diverse shapes and surroundings across the OCT B-scans, making it harder to generalize.
\par
For the same reason, the fluid estimation is also more accurate in Topcon. In the scans obtained with devices from this vendor, the patients present less deformation associated with the presence of fluid. Therefore, the model generalizes better, as these fluids seen in these scans tend to be more similar with the majority of those seen in training.
\par
In the last experiment, the fluid volumes were also estimated for OCT volumes generated or enhanced by the GAN from Experiment 3. The limitations that were faced by the segmentation model in Experiment 5, were verified in this experiment. The segmentation model did not perform as good in IRF as it did in other fluids, especially in the generated scans. However, the limiting factor in this experiment was not only the segmentation model, but mainly the generated slices that contained IRF. The generative model was not capable of representing the IRF regions as accurately as it represented the other fluids, caused by the irregular and less predictive transitions between slices seen in IRF. This, combined with the segmentation model, which performed slightly worse in this fluid, resulted in some outliers regarding the IRF volume estimation in the generated slices.
\par
Apart from these occurrences, which mainly occurred in Spectralis due to its larger inter-slice distance, the estimated fluid volumes in the enhanced OCT volumes were very similar to those in original slices. In fact, the differences between the fluid volume estimated in the enhanced dataset and the original dataset with the GT masks, were mainly associated with the performance of the segmentation model, rather than the outcomes from the generative model.
\par
Therefore, the conclusions drawn in these experiments demonstrate the strong potential of intermediate slice synthesis in OCT in generating anatomically coherent and structurally plausible images that support other tasks such as retinal fluid segmentation and, consecutively, retinal fluid volume estimation. Nevertheless, the methods applied can still be improved, such as the lack of fine detail both in segmentation and interslice generation that specially harm the representation of IRF. With further refinements, which can include the use of more advanced networks and adjusted loss functions, this approach represents a promising solution in the improvement of the confidence in the estimation of fluid volumes.
\par
With the work developed along this dissertation, we aimed to study the applicability of state-of-the-art approaches in the segmentation, generation, and volume estimation of the retinal fluids in retinal OCT images. By methodically performing experiments that allowed us to draw the conclusions shown, which include the stronger and weaker points of this methodology, we aim to provide the support needed for studies that seek to improve the results here presented. In the following subsections, the main limitations of this study are resumed and future adjustments to the applied methods are recommended, which we believe will improve the performances obtained, promoting a faster progress in the characterization of retinal fluids in OCT.

\section{Limitations}

Despite the promising results obtained in both segmentation and generative tasks, several limitations were associated with the methodology selected for this work, as they were chosen in accordance with the available time and hardware for training the models.
\par
In the segmentation task, the experiments performed utilized the base U-Net architecture, which was selected for its reliability and widespread adoption. As seen in the literature review, other methods have been developed to tackle the segmentation problems previously shown, such as incorrect segmentation of complex structures and detection of fluid beyond the retina. Recent studies approached these problems by including enhancements such as attention mechanisms, transformer-based modules, and multi-scale feature integration. However, due to the time constraints, the difficult access to these developed solutions, and the limited computational resources, these more complex approaches were not considered.
\par
Similarly, three-dimensional convolutional networks were not considered for the segmentation of the retinal fluids. Despite the accessibility to three-dimensional networks such as the 3D U-Net, the increased computational resources required, including memory and training time, and the necessity for larger datasets made their application unfeasible.
\par
The same happened with the generative models. When training a GAN, there are two networks being trained simultaneously, which increases the duration of training. This, combined with the increase in the available data, made the training of the GANs longer than the segmentation models, which conditioned the selection of the model used in the generation of intermediate slices. Despite the trend to incorporate different modules and even different networks in the generative framework as seen in the literature, the limitations in time, computational resources, and accessibility to these frameworks lead to the implementation of a simpler model.
\par
Since the calculation of the fluid volumes is inherently dependent on the quality of the segmentation masks, in Experiment 5, and also the quality of the generated slices, in Experiment 6, the obtained results in fluid volume estimation are also conditioned by the selection of the segmentation and generative models.
\par
The segmentation model also exhibited poor generalization on the unseen CHUSJ dataset, which can be largely attributed to the training data characteristics. Since the model was trained exclusively on the RETOUCH dataset, its segmentation performance was optimized for images following specific acquisition protocols and visual characteristics.
\par
In RETOUCH, a benchmark dataset in the segmentation of retinal fluid, the images obtained with the same device follow the same imaging protocol and, therefore, present similar visual characteristics between each other, except for the patient-specific  details. Consequently, when presented with images acquired using different protocols, such as those in the CHUSJ dataset, the model struggles to adapt to features different from those in which it was trained on, such as varying levels of noise and the appearance of the structures in the choroid. This lead to multiple incorrect fluid segmentations.
\par
The generation of slices using the GAN lacked the attention to the finer details. The networks sought to optimize an accurate representation of the speckle noise, more than it did to accurately represent the retinal structures, which include the retinal fluids.
\par
The network is motivated to generate similar images at a pixel level because its loss is mainly composed of parts that compare the real and generated images at this level. In fact, only the adversarial loss is a perceptual loss. Ultimately, this resulted in generated images where the speckle noise is accurately represented, but the smaller details of the retina are perceived with poor detail by its observer. This resulted in a poor generation of IRF, the fluid that appears in smaller and more irregular regions, which requires, therefore, more attention to be accurately generated. 
\par
The segmentation of this region is affected by the poor quality, as the segmentation model also finds it difficult to understand the characteristics portrayed in the generated scan.
\par
Ultimately, this affects the results in the estimation of fluid, since this estimation is only as good as the quality of the segmentation masks. Henceforth, to improve the fluid volume estimation in enhanced datasets, it is pivotal that both the segmentation and the generation models produce realistic outputs, and this can only be achieved if they are more attentive to the finer details.

\section{Future Research}

Building upon the limitations presented in the last section, there are several improvements that could be considered in the future with the goal of enhancing the fluid segmentation, the generation of slices, and, as a consequence, the fluid estimation.
\par
The experiments performed in this dissertation used state-of-the-art methods combined with innovation in the model's inputs, producing good outputs in the rough delimitation and classification of fluid. However, three main issues were detected: oversegmentation outside the retina, poor attention to detail in smaller fluids, and weak generalization in different datasets, which leave space to further work.
\par
The segmentation beyond the retina was seen to be tackled in multiple ways in the literature. One of the most common approaches is the limitation of the input to only contain the information within it. Some studies go further and instead of just limiting the input, they provide a relative distance map, which helps not only on the oversegmentation, but also on the misclassification of pixels. For these reasons, the use of a relative distance map would significantly enhance the models robustness and generalization, improving the overall Dice scores in segmentation.
\par
However, this does not improve the details in the segmentation of small structures, which is a much more complex problem. The use of a single multi-class model models makes it harder to tackle this, since it needs both global and local features to perform the segmentation. For example, when the U-Net was trained with small patches, the model did not understand the influence of the larger structures in the segmentation. The opposite happened when using larger patches, as the model did not segment the finer details correctly.
\par
To tackle this problem, a single U-Net is not enough, especially knowing that some fluids are better segmented with networks of different depth, based on their varying shapes and sizes. Therefore, other simple and state-of-the-art networks could be explored, such as the U-Net++ or the DeepLabV3+. These networks combine both shallow and deep features together, ensuring the rough segmentation of fluid and the attention to small details. Instead of changing the model completely, other attention mechanisms modules could also be added to the U-Net, such as self-attention.
\par
Lastly, generalization to external datasets remains a challenge. In the CHUSJ dataset, the model would frequently predict fluid outside the retina, which could be mitigated using retinal layer delimitation. However, the variations of the speckle noise, which vary depending on the scan acquisition protocols, impacted the segmentation quality. To improve robustness under these conditions, future works could explore data augmentation with Gaussian noise or by training the models with data from different sources to increase variability in the training data.
\par
Nevertheless, an improvement in segmentation models would transfer to a more accurate fluid estimation. The suggested changes, which also enhance the segmentation of IRF, would be particularly useful in the estimation of this fluid, which does not generalize as well as SRF and PED, due to its characteristic diversity.
\par
In the generation of intermediate slices, the GAN can be improved both in loss and in architecture, as a way to improve its weak attention to details and shift the focus from accurate representation of noise to a more accurate representation of fluid.
\par
To better perceive the wrongfully generated fluid regions that do not resemble anything seen in the other training images, the model's loss should incorporate stronger perceptual losses. One widely adopted perceptual loss is the LPIPS Learned perceptual image patch similarity), which compares the perceptual similarity between the real and the generated images, correlating well with the perception of the image by the human vision, leading the model to create B-scans that are more natural in detail. 
\par
To enhance the focus on smaller details by the generative network, some changes can be made to the generator, which is a simple U-shaped model. Similar to the suggestions in the segmentation model, attention blocks such as self-attention can be introduced in the network, promoting the focus on relevant spatial features, which improves the details in the generated slice.
\par
Similarly to what is seen in the literature, the generator could also adopt a multi-scale architecture, where images of different resolutions are output and compared, promoting both an accurate global structure, but also the needed attention to detail.
\par
Lastly, it would be interesting to study the results produced when changing the input shape from small patches to the whole image, and understand how this affects the outcomes of the generative model. The proposed suggestions could also be adapted to this bigger input shape.
\par
By improving the details in the generated B-scans by applying these suggestions, the results from the fluid volume estimation in enhanced and generated volumes would therefore improve as well, reaching values that are closer to those seen in the masks predicted in the unchanged OCT volumes. This would significantly improve the confidence in the calculated fluid volumes, especially in the Spectralis device, where the distance between slices is larger and, therefore, so is the uncertainty in the predicted volumes.
