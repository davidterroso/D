\chapter{Conclusion}\label{Conclusion}

Throughout this dissertation, two main tasks were tackled in retinal OCT: fluid segmentation and intermediate slice generation. Theses tasks were explored with the goal of improving the automatic retinal fluid characterization, which includes the segmentation, identification, and quantification of IRF, SRF, and PED. This final chapter revisits the key findings and insights obtained from the experiments performed, presenting the strengths and limitations of the proposed methods.
\par
In the first experiments, multiple approaches to the multi-class segmentation were studied. The segmentation experiments started with a baseline U-Net, used in the multi-class segmentation of fluids. While  still using this network, the input was changed, testing different patch shapes and extraction procedures, which provided an insight on how each affects the network performance.
\par
When using smaller patches, extracted predominantly from the retina, the U-Net failed to understand the anatomic boundaries and relationships that define the retina, limited by the area of the image captured in this input. Therefore, the model would often segment outside of the retina and predictions while different fluids would be attributed to similar regions.
\par
The use of larger patches, which occupied the majority of each the B-scan, significantly improved the model's performance, but did not solve all the problems seen in smaller patches. While the model better understood the limits imposed by the retinal layers and learned how to leverage these landmarks in segmentation, the segmentations lacked detail in smaller regions. This is a common issue in models trained with the full image or larger patches. The model learns the bigger features better, such as the relationships between layers and fluids, but fails to grasp the smaller details, such as small fluid regions, commonly merging them.
\par
During the extraction of these larger patches, the retinal layers would sometimes be divided in two independent patches, which also affected the perception of the retinal layers, leading to segmentations outside the retina. Therefore, the models trained with larger patches lacked both the capability of accurately segmenting the fluid and a flawless understanding the influence which the retinal layers have in segmentation.
\par
As a way to find a compromise between the large patches which lack the detail in their segmentations and the smaller patches which can not grasp the importance of the anatomic references, a different patch shape was used. These patches were larger vertically than horizontally, capturing the whole height of the image, but small enough along the horizontal direction so that the model can focus on the smaller details. Complementing this, all the images were also resized to same shape. This made the same structures appear similarly, independently of the OCT device used to obtain the B-scan.
\par
With these implementations, the results significantly improved when compared to the previous patch shapes. The model understood the relationships between the anatomic landmarks in the retina and the different fluid regions, while still performing detailed segmentations in complicated areas, with consistent performances across the multiple fluids.
\par
Still, new patch extraction methods were tested and instead of dividing each image in four disjoint vertical patches, seven and thirteen overlapping patches were extracted. When training the model in these two conditions, the model learned significantly faster, reaching is maximum potential earlier than when trained with four patches. Since in each epoch the number of images seen increased, the progress was faster. While this did not translate to a performance increase when training with thirteen patches, the results shown in when using seven patches were better.
\par
Different rotations used in the transformation of the inputs were tested, with the goal of mitigating wrong segmentations that were happening. In these segmentations, the fluids would appear in senseless anatomic positions. The model was tested with no rotation, $5^{\circ}$, and $10^{\circ}$ maximum rotation. The performance of the model significantly decreased without rotation, an important transformation in OCT, where volumes appear with different inclinations of the retina. With a maximum $5^{\circ}$ rotation, the performance was slightly worse than when using $10^{\circ}$, but it mitigated the wrong segmentations that were being tackled. For this reason, the best model with $5^{\circ}$ rotation was selected to perform the final inference in unseen data, which includes the reserved fold from the RETOUCH dataset and the entirety of the CHUSJ dataset.
\par
In Experiment 2, a binary segmentation U-Net was trained to segment each of the three fluids. The motivation behind this experiment was that by simplifying the task, the resulting segmentation would be better. However, this did not happen, as the classification of fluid in its different classes by a single model, motivated it to better learn the anatomy of the retina and its relation with the fluids.  As the model's task was changed from multi-class to binary segmentation, the model did not understand this relation as deeply as the multi-class model did. Instead, the model relied more on the pixel intensity to make its predictions. For these reasons, the model would constantly oversegment fluids, often outside the retina.
\par
In this Experiment, two different data splits were used to train each model. The first data split was the one used in multi-class segmentation, while the second was specifically made for each fluid desired to segment.
\par
The performances changed significantly depending on the split used to train the models. While the average Dice coefficients saw small improvements, the performances presented less deviation across folds since the data was distributed more uniformly. The models selected to segment unseen data were those which obtained the best segmentation performances and all of them were trained on fluid-specific splits.
\par
The first models were trained using the same loss as the multi-class, which was a combination of Dice and cross-entropy. However, these binary segmentation models were also implemented using just the cross-entropy. The results obtained in segmentation when using this loss were much worse, as the model kept predicting fluid in slices where it was not present.
\par
The performances in the unseen RETOUCH volumes were satisfying, as the results obtained with the multi-class model were better than the performance on validation, while those obtained with the binary models performed similarly to the validation, in general. These results highlight the models' robustness, since it had not been nor trained nor validated in this data.
\par
When comparing these two approaches with those in the literature and past works which were also trained and evaluated in RETOUCH, the results were satisfying. Among other disadvantages, the models implemented in these experiments are simpler than any of those to which it was compared. Yet, it achieves similar or comparable results with less training duration and complexity.
\par
In the CHUSJ, the performance was worse than in the RETOUCH dataset. While the segmentation of the fluid regions was decent and all the fluids were correctly identified, the different characteristics of the dataset lead to a poor overall performance. The different images' quality, which highlighted the choroid region, lead to the common segmentation of IRF in this region. Meanwhile, the different criteria in the segmentation of PED resulted in multiple instances of oversegmentation.
\par
Overall, the fluid segmentation experiments were pivotal to the understanding of how the input shape and the patch extraction influence the segmentation of fluid in OCT, particularly through the observation of the models' learning process. 



The results from Experiment 4 reveal the necessity of using a more complex approach to the generation task, in OCT. While the methods used in this experiment were enough to produce satisfying results in the original article, since the CT dataset used was less noisy, they are not robust enough to be applied in OCT.
\par
The capability of the U-Net to generate images is fundamentally constrained by the loss that regularizes the training. When using the MAE loss, the network can only compare the generated slices with its GT at a pixel level, without considering the general perception of the image. As a result, the U-Net produces blurry images, particularly in the regions with noise and uncertainty. In these regions, since predicting the exact noise pattern is impossible, the network learns to average the possible variations, smoothing the final output. This minimizes the MAE, at the cost of sharpness and realism.
\par
Since the speckle noise corrupts every region of the B-scans, the entirety of the output is blurred. This issue is mitigated by the addition of an adversarial loss in the GAN in Experiment 3. The adversarial noise, which corresponds to the ability to fool the discriminator network, that evaluates the perception of the image and promotes the representation of noise in the generated images, making them more similar to the real outputs. This highlights the need of a more complex and robust approach to the generation of OCT, where images are significantly affected by noise.
\par
The use of the entire image instead of the small patches, as done in Experiment 3, also contributes to the model generating images with less detail. The use of smaller patches motivates the network to focus on local structures more than the overall scan, resulting in an image richer in detail.
\par
In Experiment 5, where the volumes of each fluid in the OCT scans that compose the reserved fold were calculated, conclusions were drawn that supported the previous statements. 
\par
While the calculated volumes in the predicted masks were similar to those in the GT without presenting any extreme outlier, the model exhibits consistent oversegmentation. In most OCT volumes and particularly in IRF, the model segmented more fluid than there was in the GT. This oversegmentation is highly related with the RETOUCH dataset, in which it was trained, which promotes oversegmentation, as seen in the fluid segmentation experiments. 
\par
However, the oversegmentation in IRF is not only rooted to the dataset, but also to the model capabilities. While the shape and intensity of the SRF and PED in the training data is similar to those seen in the reserved fold as these characteristics are more consistent in these fluids, IRF presents diverse shapes and surroundings across the OCT B-scans, making it harder to generalize.
\par
For the same reason, the fluid estimation is also more accurate in Topcon. In the scans obtained with devices from this vendor, the patients present less deformation associated with the presence of fluid. Therefore, the model generalizes better, as these fluids seen in these scans tend to be more similar with the majority of those seen in training.

\section{Future Research}

Despite the satisfying results obtained in the fluid segmentation experiments, improvements can still be made in order to reach better results, building on top of the experiments performed in this dissertation. These experiments used state-of-the-art methods combined with innovation in the model's inputs, producing good outputs in the rough delimitation and classification of fluid. However, three main issues were detected: oversegmentation outside the retina, poor attention to detail in smaller fluids, and weak generalization in different datasets. Therefore, these weaknesses leave space to further work.
\par
The segmentation beyond the retina was seen to be tackled in multiple ways in the literature. One of the most common approaches is the limitation of the input to only contain the information within it. Some studies go further and instead of just limiting the input, they provide a relative distance map, which helps not only on the oversegmentation, but also on the misclassification of pixels. For these reasons, the use of a relative distance map would significantly enhance the models robustness and generalization, improving the overall Dice scores in segmentation.
\par
However, this does not improve the details in the segmentation of small structures, which is a much more complex problem. The use of a single multi-class model models makes it harder to tackle this, since it needs both global and local features to perform the segmentation. For example, when the U-Net was trained with small patches, the model did not understand the influence of the larger structures in the segmentation. The opposite happened when using larger patches, as the model did not segment the finer details correctly.
\par
To tackle this problem, a single U-Net is not enough, especially knowing that some fluids are better segmented with networks of different depth, based on their different sizes. Therefore, other simple and state-of-the-art networks could be explored, such as the U-Net++ or the DeepLabV3+. These networks combine both shallow and deep features together, ensuring the rough segmentation of fluid and the attention to small details. Instead of changing the model completely, other attention mechanisms modules could also be added to the U-Net, such as self-attention.
\par
Lastly, generalization to external datasets remains a challenge. In the CHUSJ dataset, the model would frequently predict fluid outside the retina, which could be mitigated using retinal layer delimitation. However, the variations of the speckle noise, which vary depending on the scan acquisition protocols, impacted the segmentation quality. To improve robustness under these conditions, future work could explore data augmentation with Gaussian noise or by training the models with data from different sources to increase variability in the training data.
\par
Nevertheless, an improvement in segmentation models would transfer to a more accurate fluid estimation. The suggested changes, which also enhance the segmentation of IRF, would be particularly useful in the estimation of this fluid, which does not generalize as well as SRF and PED, due to its characteristic diversity.

\section{Limitations}
Despite the promising results obtained in both segmentation and generative tasks, several limitations were associated with the methodology selected for this work. These methods were selected in accordance with the available time and hardware for training the models.
\par
In the segmentation task, the experiments performed utilized the base U-Net architecture, which was selected for its reliability and widespread adoption. As seen in the literature review and mentioned in the future research, other methods have been developed to tackle the segmentation problems previously shown, such as incorrect segmentation of complex structures and detection of fluid beyond the retina. Recent studies approached these problems by including enhancements such as attention mechanisms, transformer-based modules, and multi-scale feature integration. However, due to the time constraints, the difficult access to these developed solutions, and the limited computational resources, these approaches were not considered.
\par
