\chapter{Conclusion}\label{Conclusion}

Throughout this dissertation, two main tasks were tackled in retinal OCT: fluid segmentation and intermediate slice generation. Theses tasks were explored with the goal of improving the automatic retinal fluid characterization, which includes the segmentation, identification, and quantification of IRF, SRF, and PED. This final chapter revisits the key findings and insights obtained from the experiments performed, presenting the strengths and limitations of the proposed methods.
\par
In the first experiments, multiple approaches to multi-class segmentation were studied. The segmentation experiments started with a baseline U-Net, used in the multi-class segmentation of fluids. While  still using this network, the input was changed, testing different patch sizes and extraction procedures, which provided an insight on how each affects the network performance.
\par
When using smaller patches for training, extracted predominantly from the retina, the U-Net failed to understand the anatomical boundaries and relationships that define the retina, limited by the area captured in those patches. Therefore, the model would often segment outside the retina, and different fluids would be predicted in similar regions.
\par
The use of larger patches, which occupied the majority of each B-scan, significantly improved the model's performance, but did not solve all the problems seen in smaller patches. While the model better understood the limits imposed by the retinal layers and learned how to leverage these landmarks in segmentation, the segmentation lacked detail in smaller regions. This is a common issue in models trained with the full image or larger patches: the model learns the broader structures better, such as the relationships between layers and fluids, but fails to capture the finer details, often merging small fluid regions.
\par
During the extraction of these larger patches, the retinal layers would sometimes be split into two separate patches, which also affected the perception of the retinal layers, leading to segmentations outside the retina. Therefore, the models trained with larger patches lacked both the capability of accurately segmenting the fluid and a consistent understanding of the influence of retinal layers in segmentation.
\par
As a way to find a compromise between the large patches, which lack detail, and the small patches, which fail to capture anatomical references, a different patch size was used. These patches were larger vertically than horizontally, capturing the whole height of the image, but small enough along the horizontal direction so that the model can focus on finer details. Additionally, all images were resized to the same dimensions, ensuring that the same structures appeared consistently, regardless of the OCT device used for obtaining the B-scan.
\par
With these implementations, the results significantly improved when compared to the previous patch sizes. The model understood the relationships between the anatomical landmarks in the retina and the different fluid regions, while still performing detailed segmentations in complex areas, with consistent performances across multiple fluids.
\par
Still, new patch extraction methods were tested. Instead of dividing each image into four disjoint vertical patches, seven and thirteen overlapping patches were extracted. When training the model under these two conditions, the model learned significantly faster, reaching its maximum potential earlier than when trained with four patches. Since the number of images seen in each epoch increased, training progressed faster. While this did not translate to a performance increase when training with thirteen patches, the results obtained using seven patches were better.
\par
Different rotations used in the transformation of the training patches were tested, with the goal of mitigating incorrect segmentations. In these cases, the fluids would appear in anatomically implausible positions. The model was tested with no rotation, $5^{\circ}$ rotation, and $10^{\circ}$ maximum rotation. The model's performance of the model significantly decreased without rotation, highlighting the importance of this transformation in OCT, where volumes often appear with varying retinal inclinations. With a maximum $5^{\circ}$ rotation, the performance was slightly worse than with $10^{\circ}$, but it effectively mitigated the problematic segmentations. For this reason, the best model using $5^{\circ}$ rotation was selected to perform the final inference on unseen data, including the reserved test fold from the RETOUCH dataset and the CHUSJ dataset.
\par
In Experiment 2, a binary segmentation model based on U-Net was trained to segment each of the three fluids. The motivation behind this experiment was that by simplifying the task, the resulting segmentation might improve. However, this did not occur, as the classification of fluid in its different classes by a single model encouraged better learning of retinal anatomy and its relationship with the fluids. When the model task was changed from multi-class to binary segmentation, the model failed to understand this relationship as deeply as the multi-class segmentation model did. Instead, the model relied more on the pixel intensity to make its predictions. As a result, the model frequently oversegmented fluids, often outside the retina.
\par
In this experiment, two different data splits were used for training each model. The first split was the one used for multi-class segmentation, while the second was specifically designed for each fluid being segmented.
\par
The performances changed significantly depending on the split used for training. While the average Dice coefficients showed only small improvements, the performances presented less deviation across folds, since the data was distributed more uniformly. The models selected to segment unseen data were those that obtained the best segmentation performances, and all of them were trained using fluid-specific splits.
\par
The first models were trained using the same loss as the multi-class models, which was a combination of Dice loss and cross-entropy. However, these binary segmentation models were also implemented using just the cross-entropy. The results obtained in segmentation when using this loss were significantly worse, as the model kept predicting fluid in slices where it was not present.
\par
The performances in the unseen RETOUCH volumes were satisfactory, as the results obtained with the multi-class segmentation model were better than the performance on validation, while those obtained with the binary segmentation models performed similarly to the validation, in general. These results highlight the models' robustness, since they had not been trained or validated on this data.
\par
When comparing these two approaches with those in the literature and past works that were also trained and evaluated on RETOUCH, the results were satisfying. Among other disadvantages, the models implemented in these experiments are simpler than any of those to which they were compared. Yet, they achieved similar or comparable results with less training time and complexity.
\par
In the CHUSJ dataset, the performance was worse than in the RETOUCH dataset. While the segmentation of the fluid regions was reasonable and all the fluids were correctly identified, the different characteristics of the dataset lead to a poor overall performance. The different image qualities, which highlighted the choroid region, led to frequent segmentation of IRF in this region. Meanwhile, the different criteria in the segmentation of PED resulted in multiple instances of oversegmentation.
\par
When post-processing the predicted masks by limiting the fluid to the retinal region in the CHUSJ dataset, the model's performance improved significantly. This post-processing removed all the problematic oversegmentations in the choroid, increasing the Dice coefficient across all fluids. While IRF and SRF saw significant improvements in this metric in the slices with fluid, PED did not improve much. The segmentation of this fluid was primarily affected by the differing segmentation criteria used for image annotation in the RETOUCH and CHUSJ datasets.
\par
Overall, the fluid segmentation experiments were pivotal for understanding of how the input size and the patch extraction method influence the segmentation of fluid in OCT, particularly through the observation of the models' learning process. 
\par
The first experiment in intermediate slice generation evaluated the use of a GAN for generating a single B-scan between two consecutive real B-scans. The results from the GAN were visually very similar to the real slices, as the generated retinal layers, speckle noise, and some fluid regions were really similar to the examples it had learned from.
\par
At a pixel level, the metrics used in the comparison between images revealed significant differences. However, these values were highly influenced by the presence of speckle noise in the images, which is difficult to predict due to its random nature. Therefore, an analysis between generated images must always be complemented with a perceptual loss assessment and visual inspection, and visually, the images were really similar.
\par
However, the generation of these intermediate slices is highly dependent on the characteristics of the inputs. For example, the intermediate slices in OCT volumes obtained using the Cirrus device were easier to generate, as the distance between two consecutive slices was significantly smaller than in Spectralis. In the scans obtained with this last vendor, the distance between slices is larger, which resulted in rougher transitions between consecutive scans. This led to a poorer quality in the generated images, as the information was harder to predict.
\par
The larger distance between slices seen in Spectralis is compensated by the lower noise levels present in the scans obtained with this device when compared to others. This leads to better metrics that compare the images at a pixel level, such as PSNR and SSIM, even if it does not equate to more realistic images.
\par
The characteristics of each fluid also affect the quality of these regions in generated slices. For example, the generation of SRF, which is a fluid that usually appears as an homogeneous and well defined area that occupies a large portion of the retina, is easier to generate between slices, as its visual appearance does not change quickly in consecutive scans, and the modifications that occur in this transition are easy to predict.
\par
The generation of PED regions in intermediate slices, follows a similar pattern to that of SRF, as most of the PED regions are well-defined and of considerable size. Nevertheless, there are some cases of small PED where generation is complicated, as the boundaries of the fluid tend to be more irregular.
\par
Generating IRF in an intermediate slice is the most difficult part of slice generation. This fluid usually appears in small regions with irregular borders, which are separated by thin retinal tissue structures. Between two consecutive B-scans, even in the OCT volumes obtained with the devices with the highest inter-slice resolution, these structures rapidly change their shape and visual appearance. This makes it hard to predict the borders, size, and even if the regions are merged in the intermediate slice. This issue particularly affected the Spectralis volumes, as the larger inter-slice distance made the transitions between scans rougher and the prediction of intermediate slices more difficult.
\par
Visually, the generated IRF regions did not look natural, and some were confusing, not exhibiting the characteristics that define this fluid. Moreover, the GAN did not improve on the generation of IRF, as the network's loss components mainly targeted the differences between generated and real images at a pixel level. While this worked in the dataset from which the GAN was adapted, it does not work as well in OCT, where the images are corrupted with high levels of speckle noise. In these conditions, the model optimizes to generate the noise which is, at an human level, almost imperceptible, while not improving as much the harder regions to generate that are actually understood by the human eye, such as the IRF regions.
\par
The results from Experiment 4 reveal the necessity of using a more complex approach to the generation task, in OCT. While the methods used in this experiment were enough to produce satisfying results in the original article, since the CT dataset used was less noisy, they are not robust enough to be applied in OCT.
\par
The capability of the U-Net to generate images is fundamentally constrained by the loss that regularizes the training. When using the MAE loss, the network can only compare the generated slices with their GT at a pixel level, without considering the general perception of the image. As a result, the U-Net produces blurry images, particularly in the regions with noise and uncertainty. In these regions, since predicting the exact noise pattern is impossible, the network learns to average the possible variations, smoothing the final output. This minimizes the MAE, at the cost of sharpness and realism.
\par
Since the speckle noise corrupts every region of the B-scans, the entirety of the output is blurred. This issue is mitigated by the addition of an adversarial loss in the GAN used in Experiment 3. The adversarial loss, which corresponds to the ability to fool the discriminator network, enhances the perception of the image and promotes the representation of noise in the generated images, making them more similar to the real outputs. This highlights the need for a more complex and robust approach to the generation of OCT images, where images are significantly affected by noise.
\par
The use of the entire image instead of smaller patches for training, as done in Experiment 3, also contributes to the model generating images with less detail. The use of smaller patches motivates the network to focus on local structures more than the overall scan, resulting in images richer in detail.
\par
In Experiment 5, where the volumes of each fluid in the OCT scans that compose the reserved fold were calculated, conclusions were drawn that supported the previous statements. 
\par
While the calculated volumes in the predicted masks were similar to those in the GT without presenting any extreme outlier, the model exhibited consistent oversegmentation. In most OCT volumes and particularly in IRF, the model segmented more fluid than there was in the GT. This oversegmentation is highly related to the RETOUCH dataset, in which it was trained, which encourages oversegmentation, as seen in the fluid segmentation experiments. 
\par
However, the oversegmentation in IRF is not only attributed to the dataset, but also to the model's capabilities. While the shape and intensity of the SRF and PED in the training data is similar to those seen in the reserved fold as these characteristics are more consistent in these fluids, IRF presents diverse shapes and surroundings across the OCT B-scans, making it harder to generalize.
\par
For the same reason, the fluid estimation is also more accurate in Topcon. In the scans obtained with devices from this vendor, the patients present less retinal deformation associated with the presence of fluid. Therefore, the model generalizes better, as the fluids seen in these scans tend to be more similar to the majority of those seen during training.
\par
In the last experiment, the fluid volumes were also estimated for OCT volumes generated or enhanced by the GAN from Experiment 3. The limitations that were faced by the segmentation model in Experiment 5, were also observed in this experiment. The segmentation model did not perform as well in IRF as it did in other fluids, especially in the generated scans. However, the limiting factor in this experiment was not only the segmentation model, but mainly the generated slices that contained IRF. The generative model was not capable of representing the IRF regions as accurately as it represented the other fluid types, which was caused by the irregular and less predictive transitions between slices seen in IRF. This, combined with the segmentation model, which performed slightly worse in this fluid, resulted in some outliers regarding the IRF volume estimation in the generated slices.
\par
Apart from these occurrences, which mainly occurred in Spectralis due to its larger inter-slice distance, the estimated fluid volumes in the enhanced OCT volumes were very similar to those in original slices. In fact, the differences between the fluid volume estimated in the enhanced dataset and the original dataset with the GT masks, were mainly associated with the performance of the segmentation model, rather than the outcomes from the generative model.
\par
Therefore, the conclusions drawn in these experiments demonstrate the strong potential of intermediate slice synthesis in OCT for generating anatomically coherent and structurally plausible images that support other tasks such as retinal fluid segmentation and, consecutively, retinal fluid volume estimation. Nevertheless, the methods applied can still be improved, such as the current lack of fine detail both in segmentation and inter-slice generation, which especially harms the representation of IRF. With further refinements, which can include the use of more advanced networks and better loss functions, this approach represents a promising solution for improving the confidence in the estimation of fluid volumes.
\par
With the work developed throughout this dissertation, we aimed to study the applicability of state-of-the-art approaches for the segmentation, generation, and volume estimation of the retinal fluids in retinal OCT images. By methodically performing experiments that allowed us to draw the conclusions shown, highlighting the strengths and weaknesses of this methodology, we aim to provide the support needed for studies that seek to improve the results presented here. In the following subsections, the main limitations of this study are summarized and future adjustments to the applied methods are recommended, which we believe will improve the performances obtained, promoting a faster progress in the characterization of retinal fluids in OCT.

\section{Limitations}

Despite the promising results obtained in both segmentation and generative tasks, several limitations were associated with the methodology selected for this work, as choices were made in accordance with the available time and hardware for training the models.
\par
In the segmentation task, the experiments performed utilized the baseline U-Net architecture, which was selected for its reliability and widespread adoption. As seen in the literature review, other methods have been developed to tackle the segmentation problems previously shown, such as incorrect segmentation of complex structures and detection of fluid beyond the retina. Recent studies approached these problems by including enhancements such as attention mechanisms, transformer-based modules, and multi-scale feature integration. However, due to the time constraints, the difficult access to these developed solutions, and the limited computational resources, these more complex approaches were not considered.
\par
Similarly, three-dimensional convolutional networks were not considered for the segmentation of the retinal fluids. Despite the accessibility to three-dimensional networks such as the 3D U-Net, the increased computational resources required, including memory and training time, and the necessity for larger datasets made their application unfeasible.
\par
The same limitations applied to the generative models. When training a GAN, there are two networks being trained simultaneously, which increases the duration of training. This, combined with the increase in the available data, resulted in longer training times for the GANs compared to the segmentation models, which constrained the selection of the model used for generating intermediate slices. Despite the trend to incorporate different modules and even different networks in the generative framework as seen in the literature, the limitations in time, computational resources, and accessibility to these frameworks led to the implementation of a simpler model.
\par
Since the calculation of the fluid volumes is inherently dependent on the quality of the segmentation masks, in Experiment 5, and also the quality of the generated slices, in Experiment 6, the obtained results in fluid volume estimation are also conditioned by the selection of the segmentation and generative models.
\par
The segmentation model also exhibited poor generalization on the unseen CHUSJ dataset, which can be largely attributed to the characteristics of the training data. Since the model was trained exclusively on the RETOUCH dataset, its segmentation performance was optimized for images following specific acquisition protocols and visual characteristics.
\par
In RETOUCH, a benchmark dataset in the segmentation of retinal fluid, the images obtained with the same device follow the same imaging protocol and, therefore, present similar visual characteristics between each other, except for the patient-specific  details. Consequently, when presented with images acquired using different protocols, such as those in the CHUSJ dataset, the model struggles to adapt to features that differ from those it was trained on, such as varying noise levels and differences in choroidal structure appearance. This led to multiple incorrect fluid segmentations.
\par
The generation of slices using the GAN also lacked attention to finer details. The networks were driven to optimize the accurate representation of speckle noise more than the accurate representation of retinal structures, including retinal fluids.
\par
The network was motivated to generate similar images at a pixel level because its loss was mainly composed of parts that compare the real and generated images at this level. In fact, only the adversarial loss function as a perceptual loss. Ultimately, this resulted in generated images where speckle noise was accurately represented, but the finer details were poorly by the observer. This led to a poor generation of IRF, the fluid that appears in smaller and more irregular regions, which requires, therefore, more attention to be accurately synthesized. 
\par
The segmentation of these regions was affected by the low quality of generated images, as the segmentation model also struggled to interpret the characteristics depicted in the generated scans.
\par
Ultimately, this affects the fluid estimation results, since this estimation is only as good as the quality of the segmentation masks. Therefore, to improve fluid volume estimation in enhanced datasets, it is pivotal that both the segmentation and generative models produce realistic outputs, and this can only be achieved if they are more attentive to finer details.

\section{Future Research}

Building upon the limitations presented in the last section, there are several improvements that could be considered for future work, with the goal of enhancing fluid segmentation, slice generation, and, consequently, fluid volume estimation.
\par
The experiments performed in this dissertation used state-of-the-art methods combined with innovative input strategies, producing good outputs in the rough delimitation and classification of fluid. However, three main issues were detected: oversegmentation outside the retina, poor attention to detail in smaller fluids, and weak generalization to different datasets, which leave room for further improvement.
\par
The segmentation beyond the retina has been addressed in multiple ways in the literature. One of the most common approaches is limiting the input to only contain the information within it. Some studies go further and instead of just limiting the input, they provide a relative distance map, which helps not only with oversegmentation, but also with misclassification of pixels. For these reasons, the use of a relative distance map would significantly enhance the model's robustness and generalization, improving the overall Dice scores in segmentation. The use of retinal delimitation in the CHUSJ dataset as a post-processing method significantly improved the segmentation results, further motivating research in this area.
\par
However, this does not improve segmentation details in small structures, which is a much more complex problem. The use of a single multi-class segmentation model makes it harder to tackle this, since it needs both global and local features to perform the segmentation. For example, when the U-Net was trained with small patches, the model did not understand the influence of the larger structures in the segmentation. The opposite happened when using larger patches, as the model did not segment the finer details correctly.
\par
To tackle this problem, a single U-Net is not enough, especially considering that some fluids are better segmented with networks of different depth, based on their varying shapes and sizes. Therefore, other simple and state-of-the-art networks could be explored, such as the U-Net++ or the DeepLabV3+. These networks combine both shallow and deep features together, ensuring the rough segmentation of fluid and the attention to small details. Instead of changing the model completely, additional attention mechanisms modules could also be added to the U-Net, such as self-attention.
\par
Lastly, generalization to external datasets remains a challenge. In the CHUSJ dataset, the model would frequently predict fluid outside the retina, which was significantly mitigated by using retinal layer delimitation. However, variations in speckle noise, which depend on the scan acquisition protocols, impacted the segmentation quality. To improve robustness under these conditions, future works could explore data augmentation with Gaussian noise or by training the models with data from different sources to increase variability in the training data.
\par
Nevertheless, an improvement in segmentation models would lead to a more accurate fluid estimation. The suggested changes, which also enhance the segmentation of IRF, would be particularly useful in the estimation of this fluid, which does not generalize as well as SRF and PED, due to its characteristic diversity.
\par
In the generation of intermediate slices, the GAN can be improved both in terms of loss and architecture, to improve its weak attention to details and shift the focus from noise representation to more accurate fluid depiction.
\par
To better perceive the wrongfully generated fluid regions that do not resemble anything seen in the other training images, the model's loss should incorporate stronger perceptual loss functions. One widely adopted perceptual loss is the LPIPS (Learned Perceptual Image Patch Similarity), which compares the perceptual similarity between the real and the generated images, correlating well with human visual perception, guiding the model to create B-scans that are more natural in detail. 
\par
To enhance the focus on smaller details by the generative network, some changes can be made to the generator, which is a simple U-shaped model. Similar to the suggestions for the segmentation model, attention blocks such as self-attention can be introduced into network, promoting the focus on relevant spatial features, thus improving the details in the generated slice.
\par
Similarly to what is seen in the literature, the generator could also adopt a multi-scale architecture, where images of different resolutions are output and compared, promoting both an accurate global structure, and the needed attention to detail.
\par
Lastly, it would be interesting to study the results produced when changing the patch dimensions from small patches to the whole image, and to understand how this affects the outcomes of the generative model. The proposed suggestions could also be adapted to this larger patch size.
\par
By improving the details in the generated B-scans through these suggestions, the results from the fluid volume estimation in enhanced and generated volumes would therefore improve as well, reaching values closer to those seen on the masks predicted in the unchanged OCT volumes. This would significantly improve the confidence in the calculated fluid volumes, especially for the Spectralis device, where the distance between slices is larger and, therefore, so is the uncertainty in the predicted volumes.
