\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the number of OCT volumes, B-scans per volume, total number of B-scans, and associated macular diseases in each dataset.\relax }}{19}{table.caption.22}%
\contentsline {table}{\numberline {3.2}{\ignorespaces Layers that compose the generator and the discriminator. Each convolution is represented by Conv2d(K, OC, S), where K is the kernel size, OC is the number of output channels, and S is the stride. The same notation is used in deconvolutions, represented by TransposedConv2d. The output size is shown following C $\times $ H $\times $ W notation, where C is the number of channels, H is the height, and W is the width. The inputs have a size of $1 \times 64 \times 64$. Adapted from \blx@tocontentsinit {0}\textcite {Tran2020}.\relax }}{31}{table.caption.34}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Number of OCT volumes per vendor in each fold, considering a 5-fold split.\relax }}{36}{table.caption.35}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Device-wise distribution of OCT volumes across the four folds used for training and validation in OCT slice synthesis.\relax }}{37}{table.caption.36}%
\contentsline {table}{\numberline {4.3}{\ignorespaces Summary of the experiments performed in this dissertation.\relax }}{41}{table.caption.37}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Dice coefficients for every vendor and fluid for the runs done in Experiment 1.1. The conditions were the same for both sets except the extracted patches, which are different in every run due to the random patch extraction method used.\relax }}{43}{table.caption.38}%
\contentsline {table}{\numberline {5.2}{\ignorespaces Dice coefficients for every vendor and fluid for the runs performed in the Experiment 1.2.\relax }}{45}{table.caption.41}%
\contentsline {table}{\numberline {5.3}{\ignorespaces Dice coefficients for every vendor and fluid obtained from runs using four vertical patches extracted from each B-scan. In ``Set 4'', the models were trained for 100 epochs, while in ``Set 5'' they were trained for up to 200 epochs with a 100-epoch patience. The transformations applied in these sets were the same: horizontal flipping and a maximum rotation of $10^{\circ }$.\relax }}{47}{table.caption.44}%
\contentsline {table}{\numberline {5.4}{\ignorespaces Dice coefficients for every vendor and fluid from runs performed using seven (Runs 21 and 22) and thirteen (Runs 23 and 24) vertical patches extracted from each B-scan. Only two folds were used in order to understand the viability of using this number of patches, which are represented in the column ``P''. Runs 17 and 18 are shown here to make the comparison between the number of patches easier. The values shown in bold represent the best value for the models validated on the same fold.\relax }}{49}{table.caption.47}%
\contentsline {table}{\numberline {5.5}{\ignorespaces Dice coefficients for every vendor and fluid using seven vertical patches per image for training. The only transformation applied to these runs was horizontal flipping, without rotation. Runs 21 and 22 are also shown, promoting an easier comparison.\relax }}{52}{table.caption.51}%
\contentsline {table}{\numberline {5.6}{\ignorespaces Dice coefficients for every vendor and fluid using seven vertical patches per image for training. In these runs, horizontal flipping and $5^{\circ }$ rotation were used, instead of the usual $10^{\circ }$ rotation. The results obtained in Runs 21 and 22 are also shown, enabling a direct comparison.\relax }}{53}{table.caption.54}%
\contentsline {table}{\numberline {5.7}{\ignorespaces Dice coefficients for every vendor and fluid using seven vertical patches per image for training. The mean scores are calculated only for the B-scans that contain that fluid. For example, Cirrus IRF is the mean of the IRF Dice coefficient across all the Cirrus slices in the validation fold that contain IRF. The transformations utilized were the same as in Table \ref {tab:Experiment1.3SevenPatches5DegreeRotation}.\relax }}{54}{table.caption.55}%
\contentsline {table}{\numberline {5.8}{\ignorespaces Dice scores for every vendor and fluid using four vertical patches per image for training. The rotation applied in these runs was of $5^{\circ }$, instead of the previously used $10^{\circ }$. ``Set 7'' and the best-performing run in this set (Run 32) are also shown, promoting an easier comparison between the use of four and seven patches. The underlined values correspond to the best performances between the models trained in Run 32 and Run 33.\relax }}{55}{table.caption.57}%
\contentsline {table}{\numberline {5.9}{\ignorespaces Dice coefficients for every vendor and fluid in the reserved fold (fold 1). The first row reports the mean Dice coefficient across all slices. The second row shows the mean for slices containing the fluid specified in the respective column, while the third row shows the mean for the slices without that fluid type.\relax }}{56}{table.caption.58}%
\contentsline {table}{\numberline {5.10}{\ignorespaces IRF Dice scores for every vendor. Runs 37 to 40 utilize the multi-class 5-fold split from Experiment 1, while the Runs 41 to 44 use the fluid-specific 5-fold split. The results are presented both for every slice and for the slices which contain IRF.\relax }}{59}{table.caption.62}%
\contentsline {table}{\numberline {5.11}{\ignorespaces SRF Dice scores for every vendor. Runs 45 to 48 utilize the multi-class 5-fold split from Experiment 1, while the Runs 49 to 52 use the fluid-specific 5-fold split. The results are presented both for every slice and for the slices which present SRF.\relax }}{61}{table.caption.64}%
\contentsline {table}{\numberline {5.12}{\ignorespaces PED Dice scores for every vendor. Runs 53 to 56 utilize the multi-class 5-fold split from Experiment 1, while the Runs 57 to 60 use the fluid-specific 5-fold split. The results are presented both for every slice and for the slices which present PED.\relax }}{62}{table.caption.65}%
\contentsline {table}{\numberline {5.13}{\ignorespaces Dice scores for every vendor and fluid in the reserved fold (fold 1), using the models from Runs 42, for IRF, 52, for SRF, and 59, for PED. The first block corresponds to the results when using the priority rule merging strategy, while the second block is associated with the results when using highest probability as the merging strategy. In each block, the first row reports the mean Dice across all slices. The second row shows the mean for slices containing the fluid specified in the column, while the third shows the mean for the slices without that fluid. The values marked in bold are the best values when corresponding rows are compared, so that, for example, the row ``All'' in the first block is compared to the row of same name in the second.\relax }}{65}{table.caption.68}%
\contentsline {table}{\numberline {5.14}{\ignorespaces IRF Dice scores for every vendor, using the binary segmentation U-Net for IRF. The loss function used in Runs 61 and 62 is the weighted cross-entropy. This model was trained on two different validation folds: validation fold 0 from the multi-class 5-fold split, and validation fold 4 from the 5-fold split specific for IRF. These results can be compared, respectively, to those obtained in Runs 40 and 43. The metrics are compared between rows with the same name, so that the values in bold are the best across the four experiments.\relax }}{68}{table.caption.72}%
\contentsline {table}{\numberline {5.15}{\ignorespaces Dice scores for every vendor and fluid in the reserved fold (fold 1), using the best models from Experiment 1 and Experiment 2. In Experiment 1, the model trained on Run 32 was selected, while in Experiment 2 the chosen models were those from Runs 42, for IRF, 52, for SRF, and 59, for PED. The rows with the same name must be compared with each other.\relax }}{70}{table.caption.75}%
\contentsline {table}{\numberline {5.16}{\ignorespaces Dice scores for every fluid in ``Set 7'', ``Set 10, 12, and 14'', compared with the results obtained in the two networks by \blx@tocontentsinit {0}\textcite {Alsaih2020}.\relax }}{71}{table.caption.76}%
\contentsline {table}{\numberline {5.17}{\ignorespaces Dice scores for every vendor and fluid in Set 7, Set 10, 12, and 14, compared with the results obtained in the authors' previous implementation of the work by \blx@tocontentsinit {0}\textcite {Tennakoon2018}.\relax }}{73}{table.caption.77}%
\contentsline {table}{\numberline {5.18}{\ignorespaces Dice scores for every vendor and fluid in the CHUSJ dataset, using the best models from Experiment 1 and Experiment 2.\relax }}{74}{table.caption.78}%
\contentsline {table}{\numberline {5.19}{\ignorespaces Dice scores for every vendor and fluid in the CHUSJ dataset, using the best models from Experiment 1 and Experiment 2, while removing the fluids predicted outside the retina.\relax }}{78}{table.caption.87}%
\contentsline {table}{\numberline {5.20}{\ignorespaces PSNR and SSIM in every device, for the generated slices in every validation fold.\relax }}{81}{table.caption.90}%
\contentsline {table}{\numberline {5.21}{\ignorespaces Volume of each fluid calculated for the masks segmented in the RETOUCH (GT) and the masks segmented by the best segmentation model from Experiment 1 and 2 (P). The volumes correspond to the total fluid volume in the OCT scan, in mm$^{3}$.\relax }}{89}{table.caption.95}%
\contentsline {table}{\numberline {5.22}{\ignorespaces Volume of each fluid calculated for the masks segmented in the RETOUCH (GT) and for the masks predicted by the best segmentation model in the original RETOUCH images (P), in the generated slices (G), and when the original dataset is enhanced, combining alternating the real images with B-scans generated between them (E). The values correspond to the total fluid volume in the OCT volume, measured in mm$^{3}$, with the relative error to the GT shown in parentheses below each volume.\relax }}{91}{table.caption.97}%
\contentsline {table}{\numberline {5.23}{\ignorespaces Comparison in Dice scores for every vendor and fluid in the reserved fold (fold 1), using the best segmentation model to segment the original B-scans and their generated equivalent.\relax }}{95}{table.caption.101}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {A.1}{\ignorespaces Dice scores for every vendor and fluid from multiple runs performed in the same conditions. ``Set 1'' includes Runs 1 to 5, all validated on fold 2. Meanwhile, ``Set 2'' includes Runs 6 to 14, validated on fold 3. All runs in each set were trained under the same conditions for 100 epochs. The random transformations applied were horizontal flipping and a maximum rotation of $10^{\circ }$. This provides insight into how the inherent randomness of the U-Net affects the segmentation performance.\relax }}{114}{table.caption.106}%
