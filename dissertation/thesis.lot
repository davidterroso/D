\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Volumes, B-scans per volume, the total number of B-scans, and macular diseases in each dataset.\relax }}{19}{table.caption.21}%
\contentsline {table}{\numberline {3.2}{\ignorespaces Number of OCT volumes per vendor in each fold, considering 5-fold validation.\relax }}{20}{table.caption.22}%
\contentsline {table}{\numberline {3.3}{\ignorespaces Number of OCT volumes per device in each fold, in the four remaining folds.\relax }}{21}{table.caption.23}%
\contentsline {table}{\numberline {3.4}{\ignorespaces Layers that compose the generator and the discriminator. Each convolution is represented by Conv2d(K, OC, S), where K is the kernel size, OC is the number of output channels, and S is the stride. The same notation is used in deconvolutions, represented by TransposedConv2d. The output size is shown following C $\times $ H $\times $ W notation, where C is the number of channels, H is the height, and W is the width. The inputs have shape $1 \times 64 \times 64$. Adapted from \blx@tocontentsinit {0}\textcite {Tran2020}.\relax }}{33}{table.caption.38}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Dice scores for every vendor and fluid for the runs done in Experiment 1.1. The conditions were the same for both sets except the extracted patches that are different in every run due to the random process of extracting them.\relax }}{38}{table.caption.39}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Dice scores for every vendor and fluid for the runs done in Experiment 1.2.\relax }}{40}{table.caption.42}%
\contentsline {table}{\numberline {4.3}{\ignorespaces Dice scores for every vendor and fluid for the runs done using four vertical patches extracted from each B-scan. In ``Set 4'', the models were trained in 100 epochs, while in ``Set 5'' the models were trained on up to 200 epochs with a 100 epoch patience. The transformations applied in these sets were the same: horizontal flipping and a maximum rotation of $10^{\circ }$.\relax }}{42}{table.caption.45}%
\contentsline {table}{\numberline {4.4}{\ignorespaces Dice scores for every vendor and fluid for the runs done using seven (Runs 21 and 22) and thirteen (Runs 23 and 24) vertical patches extracted from each B-scan. Only two folds were used in order to understand the viability of using these numbers of patches, which is represented in the column ``P''. Runs 17 and 18 are shown here to make the comparison between the number of patches easier. The values shown in bold represent the best value for the models trained with the same number of patches.\relax }}{44}{table.caption.48}%
\contentsline {table}{\numberline {4.5}{\ignorespaces Dice scores for every vendor and fluid using seven vertical patches. The only transformation performed in these runs was horizontal flipping, without rotation.\relax }}{46}{table.caption.50}%
\contentsline {table}{\numberline {4.6}{\ignorespaces Dice scores for every vendor and fluid using seven vertical patches. In these runs, horizontal flipping and $5^{\circ }$ rotation were used, instead of the usual $10^{\circ }$.\relax }}{46}{table.caption.51}%
\contentsline {table}{\numberline {4.7}{\ignorespaces Dice scores for every vendor and fluid using four vertical patches. The rotation applied in these runs was of $5^{\circ }$, instead of the previously used $10^{\circ }$.\relax }}{46}{table.caption.52}%
\contentsline {table}{\numberline {4.8}{\ignorespaces Dice scores for every vendor and fluid in the reserved fold (validation fold 1). The OCT volumes contained in this folds do had not been used previously nor in training nor in validation. The inference was done using the model trained on Run 32, which was the best performing model in the runs seen in Table \ref {tab:Experiment1.3SevenPatches5DegreeRotation} and in Table \ref {tab:Experiment1.3FourPatches5DegreeRotation}.\relax }}{46}{table.caption.53}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {A.1}{\ignorespaces Dice scores for every vendor and fluid for multiple runs performed in the same conditions. ``Set 1'' resumes Run 1 to Run 5, all of which validated using fold 2. Meanwhile, ``Set 2'' resumes the Runs from 6 to 14, validated in fold 3. All the runs in each set were trained on the same conditions, for 100 epochs. The random transformations applied were horizontal flipping and maximum rotation of $10^{\circ }$ applied to it. This provides an insight of how the randomness of the U-Net affects the segmentation performance.\relax }}{55}{table.caption.56}%
