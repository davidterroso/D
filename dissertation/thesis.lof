\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces OCT B-scans (A), acquired at fixed intervals along the azimuthal axis, form a volume representation of the posterior segment of the eye (B) \blx@tocontentsinit {0}\parencite {Jain2010}.\relax }}{2}{figure.caption.9}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The three distinct fluid types on an OCT B-scan: IRF in red, SRF in green, and PED in blue \blx@tocontentsinit {0}\parencite {Bogunovic2019a}.\relax }}{3}{figure.caption.10}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces OCT scan of the retinal layers \blx@tocontentsinit {0}\parencite {Almonte2020}.\relax }}{3}{figure.caption.11}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Grouping of the articles included in the literature review.\relax }}{6}{figure.caption.12}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of a CNN architecture used for binary fluid segmentation. Image A depicts the neural network architecture, B shows the used multi-scale block, while C and D exhibit the residual convolutional blocks \blx@tocontentsinit {0}\parencite {Guo2020}.\relax }}{7}{figure.caption.13}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of a framework that includes retinal layer delimitation and construction of a relative distance map (left side). The generated map, together with the original image, serves as input to the segmentation network (denominated ICAF-Net, by the authors) \blx@tocontentsinit {0}\parencite {Tang2022}.\relax }}{9}{figure.caption.14}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces \blx@tocontentsinit {0}\textcite {Lopez2023} training process.\relax }}{11}{figure.caption.15}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Framework developed by \blx@tocontentsinit {0}\textcite {Xia2021}.\relax }}{12}{figure.caption.16}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Architecture of the method developed by \blx@tocontentsinit {0}\textcite {Zhang2024}.\relax }}{13}{figure.caption.17}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Pipeline of the methodology proposed by \blx@tocontentsinit {0}\textcite {Fang2022}.\relax }}{14}{figure.caption.18}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Pipeline that describes the RIFE framework. The student model attempts to generate the intermediate frame, while the teacher refines the frame generated by the student so that it looks more similar to the middle frame. The results from both networks are evaluated on the reconstruction loss \blx@tocontentsinit {0}\parencite {Huang2022}.\relax }}{16}{figure.caption.19}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Pipeline representing the framework developed by \blx@tocontentsinit {0}\textcite {Tran2020}. The generator that produces the intermediate frame is represented by $G$, while the discriminator is labeled $D$. The pix2pix generator is denoted by $G\_RN$ and the discriminator is represented by $D\_RN$. $x_{n-1}$ and $x_{n+1}$ respectively represent the previous and following frames of the one that is being generated, $x_{n}$. $y_{n}$ is the image generated by the first generator, while $y'_{n}$ is the image refined by the pix2pix network \blx@tocontentsinit {0}\parencite {Tran2020}.\relax }}{17}{figure.caption.20}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Retinal layers B-scans from different patients, using Cirrus (left), Topcon (middle), and Spectralis (right) devices. Despite representing the same structure, the Cirrus scan, the retinal layers appear much thicker than in the scans obtained with other vendors.\relax }}{19}{figure.caption.21}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Three B-scans with different dimensions (496 $\times $ 1024, 496 $\times $ 768, and 496 $\times $ 512 pixels, respectively) from the CHUSJ dataset. As indicated by scale bars in the bottom left corners of each B-scan, the resolution also differs across slices.\relax }}{20}{figure.caption.23}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces U-Net architecture \blx@tocontentsinit {0}\parencite {Ronneberger2015}.\relax }}{22}{figure.caption.24}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Cirrus B-scan (left), fluid masks overlay (middle) with IRF in red, SRF in green, and PED in blue, and the ROI mask overlaid in purple (right). The red bounding box signals a possible 256 $\times $ 128 patch that could be extracted.\relax }}{23}{figure.caption.25}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Cirrus B-scan with its three corresponding patches of size 496 $\times $ 512.\relax }}{24}{figure.caption.26}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Four vertical patches of dimension 496 $\times $ 128 extracted from a Cirrus B-scan.\relax }}{24}{figure.caption.27}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Seven vertical patches of dimension 496 $\times $ 128 extracted from a Cirrus B-scan.\relax }}{25}{figure.caption.28}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Thirteen vertical patches of size 496 $\times $ 128 extracted from a Cirrus B-scan.\relax }}{25}{figure.caption.29}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Scheme explaining the input data of the generative models. Each frame refers to a B-scan from an OCT volume. Extracted from \blx@tocontentsinit {0}\textcite {Tran2020}.\relax }}{27}{figure.caption.30}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Example of a GAN framework, where $\mathcal {D}$ is the discriminator and $\mathcal {G}$ is the generator \blx@tocontentsinit {0}\parencite {Creswell2018}.\relax }}{28}{figure.caption.31}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Architecture of the generator used in the GAN. It has a contracting and an expanding path, making it a U-Net like network \blx@tocontentsinit {0}\parencite {Tran2020}.\relax }}{29}{figure.caption.32}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Patches of size 64 $\times $ 64 extracted from a Cirrus B-scan previously resized to 496 $\times $ 512.\relax }}{30}{figure.caption.33}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Example of a patch extracted from a Cirrus OCT volume used in Experiment 1.1. In this patch, while the background is noticeable due to its darker shade, the choroid is harder to be identified by an observer (or a model) due to the lack of context.\relax }}{44}{figure.caption.39}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Example of a poor segmentation made by the model trained in Run 1 (right). In the left, the GT mask for the same image is shown.\relax }}{45}{figure.caption.40}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Example of the segmentation done by the model trained in Run 9 (right) and its respective GT mask (left). The B-scan segmented is the same as in Figure \ref {fig:Experiment11Segmentation}.\relax }}{46}{figure.caption.42}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Example of the segmentation done by the model trained in Run 12 (right) and its respective GT mask (left). It is noticeable that the model confuses the choroid with the retina, as segmentation of IRF and PED is performed in the choroid.\relax }}{46}{figure.caption.43}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Training and validation losses in Run 13 (left) and Run 17 (right). Despite reaching the validation loss minimum in a similar number of epochs and having comparable training and validation loss curves, the performance is really different in Table \ref {tab:Experiment1.3FourPatches}.\relax }}{48}{figure.caption.45}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Predicted masks by the models trained on Run 16 (middle) and Run 20 (right) and their respective GT (left).\relax }}{49}{figure.caption.46}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Training and validation losses for models validated on fold 2. The curve on the left are from the model trained on Run 17 with four patches, while the middle curves are from the model trained on Run 21, with seven patches. The curves on the right are from the model trained on Run 23, with thirteen patches.\relax }}{50}{figure.caption.48}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Segmentation errors in Cirrus B-scans. The GT fluid masks, seen on the left, show a large region of PED fluid. Meanwhile, the predictions made by the models trained in Run 21 and 23, which respectively correspond to the B-scans in the middle and right, classify the center of this region as IRF.\relax }}{51}{figure.caption.49}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Transformations applied to a vertical patch of a Cirrus B-scan. The original vertical patch (left) can be rotated a maximum of $10^{\circ }$, a transformation that is shown in the middle image. The image on the right represents the combination of a $10^{\circ }$ rotation and an horizontal flip. It is seen that the rotation transform pads a significant portion of the image, reducing the information in the input.\relax }}{51}{figure.caption.50}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Example of two B-scans in which the retina is oriented differently. The retina present in the right image is significantly more inclined. Both B-scans were obtained using a Cirrus device and do not present fluid.\relax }}{53}{figure.caption.52}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Segmentation errors in Cirrus B-scans when trained without random rotations, in the same B-scan as in Figure \ref {fig:CirrusSegmentationErrors}. In the left, the GT masks are shown, while in the right the predicted masks are exhibited.\relax }}{53}{figure.caption.53}%
\contentsline {figure}{\numberline {5.12}{\ignorespaces Predictions by the models trained using $5^{\circ }$ (middle image) and $10^{\circ }$ (right image) rotations, compared with the respective GT (left image).\relax }}{55}{figure.caption.56}%
\contentsline {figure}{\numberline {5.13}{\ignorespaces Predictions by the model trained on Run 32 in unseen Cirrus volumes of fold 1.\relax }}{57}{figure.caption.59}%
\contentsline {figure}{\numberline {5.14}{\ignorespaces Predictions by the model trained on Run 32 in unseen Spectralis volumes of fold 1.\relax }}{57}{figure.caption.60}%
\contentsline {figure}{\numberline {5.15}{\ignorespaces Predictions by the model trained on Run 32 in unseen Topcon volumes of fold 1.\relax }}{58}{figure.caption.61}%
\contentsline {figure}{\numberline {5.16}{\ignorespaces Predictions (right) performed by the binary IRF segmentation model and their respective GT (left). The prediction represented in the top show an example of an accurate IRF segmentation, while the bottom prediction reveals an oversegmentation in a slice that does not contain fluid.\relax }}{60}{figure.caption.63}%
\contentsline {figure}{\numberline {5.17}{\ignorespaces Predictions (right) performed by the binary PED segmentation model and their respective GT (left). In top right, the model predicted PED fluid in a region where it does not exist. The bottom images show the undersegmentation of the model trained in Run 59. While this model is capable of detecting the fluid location, it fails to segment its entirety.\relax }}{63}{figure.caption.66}%
\contentsline {figure}{\numberline {5.18}{\ignorespaces Comparison between the priority (left) and probability (right) merging approach.\relax }}{64}{figure.caption.67}%
\contentsline {figure}{\numberline {5.19}{\ignorespaces Predictions by the models trained on Run 42, 52, and 59 in unseen Cirrus volumes of fold 1 (last row), contrasting with predictions made by the multi-class model from Experiment 1.\relax }}{66}{figure.caption.69}%
\contentsline {figure}{\numberline {5.20}{\ignorespaces Predictions by the models trained on Run 42, 52, and 59 in unseen Spectralis volumes of fold 1 (last row), contrasting with predictions made by the multi-class model from Experiment 1.\relax }}{67}{figure.caption.70}%
\contentsline {figure}{\numberline {5.21}{\ignorespaces Predictions by the models trained on Run 42, 52, and 59 in unseen Topcon volumes of fold 1 (last row), contrasting with predictions made by the multi-class model from Experiment 1.\relax }}{67}{figure.caption.71}%
\contentsline {figure}{\numberline {5.22}{\ignorespaces Small oversegmentation performed in a Spectralis B-scan. This type of predictions appear commonly across multiple B-scans.\relax }}{69}{figure.caption.73}%
\contentsline {figure}{\numberline {5.23}{\ignorespaces Cirrus IRF segmentation performed by a model trained using BCE shown on the right, compared to its GT on the left. While this example has a decent segmentation, significant oversegmentation is seen on the top left of the image.\relax }}{69}{figure.caption.74}%
\contentsline {figure}{\numberline {5.24}{\ignorespaces Three B-scans with characteristics different from those in RETOUCH. The left scan shows definition on the choroid, while the middle and right scan present an odd orientation and significant noise.\relax }}{74}{figure.caption.79}%
\contentsline {figure}{\numberline {5.25}{\ignorespaces Significantly different segmentations (middle images) of the same B-scan (left image) by different evaluators, in the RETOUCH dataset. The mask seen in cyan in the last image represents the final fluid regions when considering both evaluations \blx@tocontentsinit {0}\parencite {Bogunovic2019b}.\relax }}{75}{figure.caption.80}%
\contentsline {figure}{\numberline {5.26}{\ignorespaces Visually similar PED regions are segmented in one dataset but not on the other. This ambiguous segmentation can translate to worse performances in the model testing.\relax }}{75}{figure.caption.81}%
\contentsline {figure}{\numberline {5.27}{\ignorespaces SRF and PED segmentation in two B-scans from CHUSJ. The regions segmented by the model as PED could also be considered fluid, depending on the criteria applied.\relax }}{76}{figure.caption.82}%
\contentsline {figure}{\numberline {5.28}{\ignorespaces SRF and PED segmentation (right) in a B-scan from CHUSJ, compared to its GT (left). This shows the tendency to oversegmentation by the model.\relax }}{76}{figure.caption.83}%
\contentsline {figure}{\numberline {5.29}{\ignorespaces Segmentation of IRF regions (right) compared to its respective GT (left). The prediction of fluid regions in the choroid is also noticeable.\relax }}{77}{figure.caption.84}%
\contentsline {figure}{\numberline {5.30}{\ignorespaces Segmentation of multiple fluids in the choroid region of the B-scans. This is performed often, significantly reducing the Dice coefficients seen in Table \ref {tab:CHUSJSegmentationResults}.\relax }}{78}{figure.caption.85}%
\contentsline {figure}{\numberline {5.31}{\ignorespaces Fluid segmentations performed by the multi-class model in noisy B-scans from the CHUSJ dataset.\relax }}{79}{figure.caption.86}%
\contentsline {figure}{\numberline {5.32}{\ignorespaces Example of two pairs of generated slices, one from Cirrus and one from Spectralis, which capture the retinal layers.\relax }}{81}{figure.caption.88}%
\contentsline {figure}{\numberline {5.33}{\ignorespaces Surrounding slices of the B-scans generated in Figure \ref {fig:GeneratedSlicesCirrusVsSpectralis}.\relax }}{82}{figure.caption.89}%
\contentsline {figure}{\numberline {5.34}{\ignorespaces Examples of generated B-scans with fluid.\relax }}{83}{figure.caption.90}%
\contentsline {figure}{\numberline {5.35}{\ignorespaces B-scans generated using the U-Net. The images on the left show the original B-scan while those on the left are the corresponding generated slices.\relax }}{84}{figure.caption.91}%
\contentsline {figure}{\numberline {5.36}{\ignorespaces Volume estimated for each fluid in OCT volume using the masks predicted by the segmentation model compared with the volumes calculated using the GT masks. The OCT scans considered are the same as in \ref {tab:FluidVolumesExperiment5} and the gray line marks the points in which the fluid volume in the GT is equal to the predicted.\relax }}{87}{figure.caption.93}%
\contentsline {figure}{\numberline {5.37}{\ignorespaces Volume estimated for each fluid in the OCT volumes using the GT masks, compared with the volumes estimated in columns ``P'', ``G'', and ``E''.\relax }}{90}{figure.caption.95}%
\contentsline {figure}{\numberline {5.38}{\ignorespaces Volume estimated for each fluid in the OCT volumes using the masks predicted by the segmentation model compared with the volumes calculated using the masks from columns ``G'' and ``E''.\relax }}{91}{figure.caption.96}%
\contentsline {figure}{\numberline {5.39}{\ignorespaces Volume estimated for each fluid in the OCT volumes using the masks predicted by the segmentation model in the generated dataset, compared with the volumes calculated in the enhanced dataset.\relax }}{91}{figure.caption.97}%
\contentsline {figure}{\numberline {5.40}{\ignorespaces Example of IRF segmentation in the enhanced volume TRAIN039.\relax }}{93}{figure.caption.99}%
\contentsline {figure}{\numberline {5.41}{\ignorespaces Example of SRF and PED segmentation in the enhanced volume TRAIN012.\relax }}{94}{figure.caption.100}%
\addvspace {10\p@ }
\addvspace {10\p@ }
