\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces OCT B-scans (A), acquired at fixed intervals along the azimuthal axis, form a volume representation of the posterior segment of the eye (B) \blx@tocontentsinit {0}\parencite {Jain2010}.\relax }}{2}{figure.caption.9}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The three distinct fluid types on an OCT B-scan: IRF in red, SRF in green, and PED in blue \blx@tocontentsinit {0}\parencite {Bogunovic2019a}.\relax }}{3}{figure.caption.10}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces OCT scan of the retinal layers \blx@tocontentsinit {0}\parencite {Almonte2020}.\relax }}{3}{figure.caption.11}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Grouping of the articles included in the literature review.\relax }}{6}{figure.caption.12}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of a CNN architecture used for binary fluid segmentation. Image A depicts the neural network architecture, B shows the used multi-scale block, while C and D exhibit the residual convolutional blocks \blx@tocontentsinit {0}\parencite {Guo2020}.\relax }}{7}{figure.caption.13}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of a framework that includes retinal layer delimitation and construction of a relative distance map (left side). The generated map, together with the original image, serves as input to the segmentation network (denominated ICAF-Net, by the authors) \blx@tocontentsinit {0}\parencite {Tang2022}.\relax }}{9}{figure.caption.14}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces \blx@tocontentsinit {0}\textcite {Lopez2023} training process.\relax }}{11}{figure.caption.15}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Framework developed by \blx@tocontentsinit {0}\textcite {Xia2021}.\relax }}{12}{figure.caption.16}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Architecture of the method developed by \blx@tocontentsinit {0}\textcite {Zhang2024}.\relax }}{13}{figure.caption.17}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Pipeline of the methodology proposed by \blx@tocontentsinit {0}\textcite {Fang2022}.\relax }}{14}{figure.caption.18}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Pipeline that describes the RIFE framework. The student model attempts to generate the intermediate frame, while the teacher refines the frame generated by the student so that it looks more similar to the middle frame. The results from both networks are evaluated on the reconstruction loss \blx@tocontentsinit {0}\parencite {Huang2022}.\relax }}{16}{figure.caption.19}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Pipeline representing the framework developed by \blx@tocontentsinit {0}\textcite {Tran2020}. The generator that produces the intermediate frame is represented by $G$, while the discriminator is labeled $D$. The pix2pix generator is denoted by $G\_RN$ and the discriminator is represented by $D\_RN$. $x_{n-1}$ and $x_{n+1}$ respectively represent the previous and following frames of the one that is being generated, $x_{n}$. $y_{n}$ is the image generated by the first generator, while $y'_{n}$ is the image refined by the pix2pix network \blx@tocontentsinit {0}\parencite {Tran2020}.\relax }}{17}{figure.caption.20}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Retinal layers B-scans from different patients, using Cirrus (left), Topcon (middle), and Spectralis (right) devices. Despite representing the same structure, the Cirrus scan, the retinal layers appear much thicker than in the scans obtained with other vendors.\relax }}{19}{figure.caption.21}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Three B-scans with different dimensions (496 $\times $ 1024, 496 $\times $ 768, and 496 $\times $ 512 pixels, respectively) from the CHUSJ dataset. As indicated by scale bars in the bottom left corners of each B-scan, the resolution also differs across slices.\relax }}{20}{figure.caption.23}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces U-Net architecture \blx@tocontentsinit {0}\parencite {Ronneberger2015}.\relax }}{22}{figure.caption.24}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Cirrus B-scan (left), fluid masks overlay (middle) with IRF in red, SRF in green, and PED in blue, and the ROI mask overlaid in purple (right). The red bounding box signals a possible 256 $\times $ 128 patch that could be extracted.\relax }}{23}{figure.caption.25}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Cirrus B-scan with its three corresponding patches of size 496 $\times $ 512.\relax }}{24}{figure.caption.26}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Four vertical patches of dimension 496 $\times $ 128 extracted from a Cirrus B-scan.\relax }}{24}{figure.caption.27}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Seven vertical patches of dimension 496 $\times $ 128 extracted from a Cirrus B-scan.\relax }}{25}{figure.caption.28}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Thirteen vertical patches of size 496 $\times $ 128 extracted from a Cirrus B-scan.\relax }}{25}{figure.caption.29}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Scheme explaining the input data of the generative models. Each frame refers to a B-scan from an OCT volume. Extracted from \blx@tocontentsinit {0}\textcite {Tran2020}.\relax }}{27}{figure.caption.30}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Example of a GAN framework, where $\mathcal {D}$ is the discriminator and $\mathcal {G}$ is the generator \blx@tocontentsinit {0}\parencite {Creswell2018}.\relax }}{28}{figure.caption.31}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Architecture of the generator used in the GAN. It has a contracting and an expanding path, making it a U-Net like network \blx@tocontentsinit {0}\parencite {Tran2020}.\relax }}{29}{figure.caption.32}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Patches of size 64 $\times $ 64 extracted from a Cirrus B-scan previously resized to 496 $\times $ 512.\relax }}{30}{figure.caption.33}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Example of a patch extracted from a Cirrus OCT volume used in Experiment 1.1. In this patch, the background is distinguishable due to its darker intensity; however, the choroid is harder to be identified by an observer (or a model) due to the lack of anatomical context.\relax }}{44}{figure.caption.39}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Example of a poor segmentation made by the model trained in Run 1 (right). In the left, the GT mask for the same image is shown. IRF is represented in red, SRF in green, and PED in blue.\relax }}{44}{figure.caption.40}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Example of the segmentation performed by the model trained in Run 9 (right) and its respective GT mask (left). The OCT B-scan segmented is the same as in Figure \ref {fig:Experiment11Segmentation}, and the predicted IRF is represented in red.\relax }}{46}{figure.caption.42}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Example of the segmentation performed by the model trained in Run 12 (right) and its respective GT mask (left). It is evident that the model confuses the choroid with the retina, as it segments IRF and PED within the choroid. IRF is represented in red, SRF in green, and PED in blue.\relax }}{47}{figure.caption.43}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Training and validation loss curves for Run 13 (left two plots) and Run 17 (right two plots). Despite reaching the minimum validation loss at a similar number of epochs and presenting comparable training and validation loss curves, the final performance, shown in Table \ref {tab:Experiment1.3FourPatches}, differs significantly.\relax }}{48}{figure.caption.45}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Predicted masks by the models trained on Run 16 (middle) and Run 20 (right) and their respective GT (left). SRF is represented in green and PED in blue.\relax }}{49}{figure.caption.46}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Validation loss curves for models validated on fold 2. The curve on the left is from the model trained in Run 17 with four patches, while the middle curve is from the model trained in Run 21 with seven patches. The curve on the right is from the model trained in Run 23 with thirteen patches.\relax }}{50}{figure.caption.48}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Segmentation errors in Cirrus B-scans. The GT fluid mask, seen on the left, shows a large region of PED fluid. Meanwhile, the predictions made by the models trained in Run 21 and 23, which respectively correspond to the B-scans in the middle and right, classify the center of this region as IRF. IRF is represented in red, SRF in green, and PED in blue.\relax }}{51}{figure.caption.49}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Transformations applied to a vertical patch of a Cirrus B-scan. The original vertical patch (left) can be rotated a maximum of $10^{\circ }$, a transformation that is shown in the middle image. The image on the right represents the combination of a $10^{\circ }$ rotation and an horizontal flip. It is seen that the rotation transform pads a significant portion of the image, reducing the information in the input.\relax }}{51}{figure.caption.50}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Example of two OCT B-scans in which the retina is oriented differently. The retina in the image on the right is significantly more inclined. Both B-scans were obtained using a Cirrus device and do not present fluid.\relax }}{53}{figure.caption.52}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Segmentation errors in Cirrus B-scans when trained without random rotations, using the same B-scan as in Figure \ref {fig:CirrusSegmentationErrors}. On the left, the GT mask is shown, while on the right, the predicted masks are displayed. IRF is represented in red, SRF in green, and PED in blue.\relax }}{53}{figure.caption.53}%
\contentsline {figure}{\numberline {5.12}{\ignorespaces Predictions by the models trained using $5^{\circ }$ (middle image) and $10^{\circ }$ (right image) rotations, compared with the respective GT (left image). IRF is represented in red, SRF in green, and PED in blue.\relax }}{55}{figure.caption.56}%
\contentsline {figure}{\numberline {5.13}{\ignorespaces Predictions made by the model trained on Run 32 on unseen Cirrus volumes of fold 1. IRF is represented in red, SRF in green, and PED in blue.\relax }}{57}{figure.caption.59}%
\contentsline {figure}{\numberline {5.14}{\ignorespaces Predictions made by the model trained on Run 32 on unseen Spectralis volumes of fold 1. IRF is represented in red, SRF in green, and PED in blue.\relax }}{58}{figure.caption.60}%
\contentsline {figure}{\numberline {5.15}{\ignorespaces Predictions made by the model trained on Run 32 on unseen Topcon volumes of fold 1. IRF is represented in red, SRF in green, and PED in blue.\relax }}{58}{figure.caption.61}%
\contentsline {figure}{\numberline {5.16}{\ignorespaces Predictions (right) made by the binary IRF segmentation model and their respective GT (left). The prediction represented in the top shows an example of an accurate IRF segmentation, represented in red, while the bottom prediction reveals an oversegmentation in a slice that does not contain fluid.\relax }}{85}{figure.caption.63}%
\contentsline {figure}{\numberline {5.17}{\ignorespaces Predictions (right) made by the binary PED segmentation model and their respective GT (left). In the top-right image, the model predicted PED fluid, represented in blue, in a region where it does not exist. The bottom images show undersegmentation by the model trained in Run 59. While this model is capable of detecting the fluid's location, it fails to segment it completely.\relax }}{86}{figure.caption.66}%
\contentsline {figure}{\numberline {5.18}{\ignorespaces Comparison between the priority (left) and probability (right) merging approach. IRF is represented in red, SRF in green, and PED in blue.\relax }}{86}{figure.caption.67}%
\contentsline {figure}{\numberline {5.19}{\ignorespaces Predictions made by the models trained in Run 42, 52, and 59 on unseen Cirrus volumes from fold 1 (last row), contrasted with the predictions made by the multi-class model from Experiment 1. IRF is represented in red, SRF in green, and PED in blue.\relax }}{87}{figure.caption.69}%
\contentsline {figure}{\numberline {5.20}{\ignorespaces Predictions made by the models trained in Run 42, 52, and 59 on unseen Spectralis volumes from fold 1 (last row), contrasting with the predictions made by the multi-class segmentation model from Experiment 1. IRF is represented in red, SRF in green, and PED in blue.\relax }}{87}{figure.caption.70}%
\contentsline {figure}{\numberline {5.21}{\ignorespaces Predictions made by the models trained in Run 42, 52, and 59 on unseen Topcon volumes of fold 1 (last row), contrasting with predictions made by the multi-class segmentation model from Experiment 1. IRF is represented in red, SRF in green, and PED in blue.\relax }}{88}{figure.caption.71}%
\contentsline {figure}{\numberline {5.22}{\ignorespaces Small IRF (red) oversegmentation performed in a Spectralis B-scan. This type of predictions appear commonly across multiple B-scans.\relax }}{88}{figure.caption.73}%
\contentsline {figure}{\numberline {5.23}{\ignorespaces IRF (red) segmentation in a Cirrus slice, represented in red, performed by a model trained using BCE (right). The corresponding GT is shown on the left. Although this example presents a decent segmentation, significant oversegmentation is seen on the top left of the image.\relax }}{89}{figure.caption.74}%
\contentsline {figure}{\numberline {5.24}{\ignorespaces Three B-scans from the CHUSJ dataset with characteristics different from those in RETOUCH. The left scan shows definition on the choroid, while the middle and right scans present an odd orientation and significant noise.\relax }}{89}{figure.caption.79}%
\contentsline {figure}{\numberline {5.25}{\ignorespaces Significantly different segmentations (middle images) of the same B-scan (left image) by different evaluators in the RETOUCH dataset. IRF is represented in red, SRF in green, and PED in blue. The regions shown in cyan in the rightmost image represent the regions where the two evaluators diverge \blx@tocontentsinit {0}\parencite {Bogunovic2019b}.\relax }}{89}{figure.caption.80}%
\contentsline {figure}{\numberline {5.26}{\ignorespaces Visually similar PED regions are segmented in one dataset but not in the other. These inconsistencies can translate to lead to poorer model performances during testing. IRF is represented in red, SRF in green, and PED in blue.\relax }}{90}{figure.caption.81}%
\contentsline {figure}{\numberline {5.27}{\ignorespaces SRF and PED segmentation in two OCT B-scans from CHUSJ. The regions segmented by the model as PED were not considered in the GT, but present characteristics similar to regions segmented as PED in the RETOUCH dataset (used for training).\relax }}{90}{figure.caption.82}%
\contentsline {figure}{\numberline {5.28}{\ignorespaces SRF (green) and PED (blue) segmentations (right) in a B-scan from CHUSJ, compared to its GT (left). This shows the model's tendency to oversegmentation.\relax }}{91}{figure.caption.83}%
\contentsline {figure}{\numberline {5.29}{\ignorespaces Segmentation of IRF regions (right) compared to its respective GT (left). The prediction of fluid regions in the choroid is also noticeable. IRF is represented in red, SRF in green, and PED in blue.\relax }}{91}{figure.caption.84}%
\contentsline {figure}{\numberline {5.30}{\ignorespaces Segmentation of multiple fluids in the choroid region of the B-scans. It is a common problem and significantly reduces the Dice coefficients, as reported in Table \ref {tab:CHUSJSegmentationResults}. IRF is represented in red, SRF in green, and PED in blue.\relax }}{91}{figure.caption.85}%
\contentsline {figure}{\numberline {5.31}{\ignorespaces Fluid segmentations performed by the multi-class model in noisy B-scans from the CHUSJ dataset. IRF is represented in red, SRF in green, and PED in blue.\relax }}{92}{figure.caption.86}%
\contentsline {figure}{\numberline {5.32}{\ignorespaces Processed fluid segmentations performed by the multi-class model in the examples of Figures \ref {fig:CHUSJIRFSegmentation} and \ref {fig:CHUSJChoroidSegmentation}. IRF is represented in red, SRF in green, and PED in blue.\relax }}{93}{figure.caption.88}%
\contentsline {figure}{\numberline {5.33}{\ignorespaces Processed fluid segmentations performed by the multi-class model, in the noisy scans shown in Figure \ref {fig:CHUSJSegmentationOnNoisyScans}. IRF is represented in red, SRF in green, and PED in blue.\relax }}{94}{figure.caption.89}%
\contentsline {figure}{\numberline {5.34}{\ignorespaces Example of two pairs of generated slices, one from Cirrus and one from Spectralis, which capture the retinal layers.\relax }}{95}{figure.caption.91}%
\contentsline {figure}{\numberline {5.35}{\ignorespaces Surrounding slices of the B-scans generated in Figure \ref {fig:GeneratedSlicesCirrusVsSpectralis}.\relax }}{95}{figure.caption.92}%
\contentsline {figure}{\numberline {5.36}{\ignorespaces Examples of generated B-scans with fluid.\relax }}{96}{figure.caption.93}%
\contentsline {figure}{\numberline {5.37}{\ignorespaces B-scans generated using the U-Net. The images on the left show the original B-scan while those on the left are the corresponding generated slices.\relax }}{96}{figure.caption.94}%
\contentsline {figure}{\numberline {5.38}{\ignorespaces Volume estimated for each fluid in OCT volume using the masks predicted by the segmentation model compared with the volumes calculated using the GT masks. The OCT scans considered are the same as in \ref {tab:FluidVolumesExperiment5} and the gray line marks the points in which the fluid volume in the GT is equal to the predicted.\relax }}{97}{figure.caption.96}%
\contentsline {figure}{\numberline {5.39}{\ignorespaces Volume estimated for each fluid in the OCT volumes using the GT masks, compared with the volumes estimated in columns ``P'', ``G'', and ``E''.\relax }}{98}{figure.caption.98}%
\contentsline {figure}{\numberline {5.40}{\ignorespaces Volume estimated for each fluid in the OCT volumes using the masks predicted by the segmentation model compared with the volumes calculated using the masks from columns ``G'' and ``E''.\relax }}{99}{figure.caption.99}%
\contentsline {figure}{\numberline {5.41}{\ignorespaces Volume estimated for each fluid in the OCT volumes using the masks predicted by the segmentation model in the generated dataset, compared with the volumes calculated in the enhanced dataset.\relax }}{99}{figure.caption.100}%
\contentsline {figure}{\numberline {5.42}{\ignorespaces Example of IRF (red) segmentation in the enhanced volume TRAIN039.\relax }}{100}{figure.caption.102}%
\contentsline {figure}{\numberline {5.43}{\ignorespaces Example of SRF (green) and PED (blue) segmentation in the enhanced volume TRAIN012.\relax }}{100}{figure.caption.103}%
\addvspace {10\p@ }
\addvspace {10\p@ }
