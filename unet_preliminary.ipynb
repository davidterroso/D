{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Tuning With Random Split\n",
    "\n",
    "The split consisted of: \n",
    "- 2 folds for training\n",
    "- 1 fold for validation\n",
    "- 1 fold for testing\n",
    "- 1 fold unused\n",
    "\n",
    "The splits were performed randomly as done in [here](https://github.com/davidterroso/D/blob/ab49616b7fe52ca1c68be56b24af7f6d51542f96/init/folds_split.py).\n",
    "The file used to train has been changed. The older version of the train file can be seen [here](https://github.com/davidterroso/D/blob/ab49616b7fe52ca1c68be56b24af7f6d51542f96/train.py). In the older version, the split was done as seen above, while in the current version it is seen as in [here](unet.ipynb). The code below presents the equivelent hyperparameters used to perform the runs in the current version of the train file. However, it does not take into account the changes in the way the splits were made. \n",
    "The files related with the runs below have been eliminated from the folder and, therefore, have to be accessed through a link to the commit on the GitHub.\n",
    "\n",
    "To check the final results of every run, please check [this section](#overallresults)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 1*\n",
    "- Epochs: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run1\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,\n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "The first run. While training loss slowly goes down to 0.26, the validation loss never crosses below the 0.39. Perhaps, a learning rate scheduler would be useful to implement in this conditions, to check if lower validation score is possible. Similarly, implementing this experiment with a lower learning rate may be useful to find a possible lower minimum error, with the expense of requiring a larger number of epochs in training. Also, it is important to mention that in this experiment, the data was obtained assycronously, unlike what was initially proposed, that recommended extracting random patches in every epoch. This later implementation must be compared against the assyncronous, to evaluate the necessity of the patch extraction every epoch. It is important to note that the patch extraction is very time consuming, with the training process in an epoch taking as long as 15 minutes (without it, it takes 2 minutes).\n",
    "\n",
    "![Training Error in Run1](./unet_preliminary_imgs/Run1_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run1_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run1_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run1_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run1_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run1_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.235  |  0.176  |  0.191  |\n",
    "| Spectralis |  0.515  |  0.701  |  0.470  |\n",
    "| Topcon     |  0.401  |  0.100  |  0.123  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.355  |  0.371  |  0.223  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 2*\n",
    "- Epochs: 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run1\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "In this run, the number of epochs was increased from 100 to 200, in order to verify whether it is possible to obtain better results in a longer run with the previous hyper parameters. The best model obtained in this training performs really similar to the previous, taking twice the same amount of time.\n",
    "\n",
    "![Training Error in Run2](./unet_preliminary_imgs/Run2_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run2_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run2_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run2_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run2_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run2_class_dice.csv). The differences when compared with the previous run come from the randomness associated with patch extraction.\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.156  |  0.106  |  0.177  |\n",
    "| Spectralis |  0.463  |  0.660  |  0.411  |\n",
    "| Topcon     |  0.462  |  0.014  |  0.139  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.272  |  0.218  |  0.215  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 3*\n",
    "- Patch extraction was done every epoch without patch dropping, instead of once before the training\n",
    "- No patch dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run3\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=False,\n",
    "    patch_dropping=False,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "The model was trained for a longer period of time, since it required patch extraction before training each epoch. It requires an increased amount of training time since larger batches are considered (without dropping patches) and the extraction of images makes the process longer.\n",
    "\n",
    "![Training Error in Run3](./unet_preliminary_imgs/Run3_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run3_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run3_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run3_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run3_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run3_class_dice.csv).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.230  |  0.103  |  0.217  |\n",
    "| Spectralis |  0.582  |  0.751  |  0.491  |\n",
    "| Topcon     |  0.371  |  0.041  |  0.226  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.363  |  0.275  |  0.306  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 4*\n",
    "- Syncronous patch extraction with patch dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run4\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=False,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "The results obtained regarding the loss during training and validation were really similar to those obtained with assyncronous patch extraction. This begs the question whether the improvements in last training were due to the randomization that patch extraction done in every epoch brings or due to the larger quantity of data available due to no employing patch dropout. One future experiment may be exactly that: re-run the first experiment without patch dropout. \n",
    "\n",
    "![Training Error in Run4](./unet_preliminary_imgs/Run4_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run4_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run4_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run4_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run4_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run4_class_dice.csv). The fluid metrics got worse as patches were dropped, when compared to the previous experiment.\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.191  |  0.010  |  0.193  |\n",
    "| Spectralis |  0.513  |  0.690  |  0.436  |\n",
    "| Topcon     |  0.530  |  0.075  |  0.233  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.343  |  0.191  |  0.278  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 5*\n",
    "- Repetion of Run 1 without dropping patches  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run5\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=False,\n",
    "    patch_dropping=False,\n",
    "    drop_prob=0.75,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "When compared with other runs, it is seen that the validation error becomes smaller than when runs use patch dropping. That happens because 50% of the loss used to train this network corresponds to the Dice loss in the background class. Since no patch dropping happens, the network gets better at segmenting the slices that do not present fluid, thus improving the loss values. \n",
    "\n",
    "![Training Error in Run5](./unet_preliminary_imgs/Run5_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run5_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run5_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run5_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run5_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run5_class_dice.csv). Got a worse performance than in *Run1*, when patches were dropped.\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.133  |  0.069  |  0.184  |\n",
    "| Spectralis |  0.592  |  0.703  |  0.428  |\n",
    "| Topcon     |  0.274  |  0.014  |  0.229  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.259  |  0.189  |  0.251  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 6*\n",
    "- Repetion of Run 1, dropping 50% of the non-pathological patches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run6\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.50,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Similarly to the last runs, a decreased loss value is obtained in models that handle more images with no pathology/no fluid to segment.\n",
    "\n",
    "![Training Error in Run6](./unet_preliminary_imgs/Run6_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run6_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run6_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run6_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run6_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run6_class_dice.csv). All performances were worse than the previous.\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.093  |  0.065  |  0.190  |\n",
    "| Spectralis |  0.602  |  0.793  |  0.313  |\n",
    "| Topcon     |  0.136  |  0.033  |  0.087  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.222  |  0.165  |  0.180  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 7*\n",
    "- Repetion of Run 1, dropping 25% of the non-pathological patches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run7\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=0.25,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The results in training achieved a better loss due to a higher quantity of non-pathological scans when compared to the previous models. \n",
    "\n",
    "![Training Error in Run7](./unet_preliminary_imgs/Run7_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run7_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run7_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run7_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run7_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run7_class_dice.csv). The inclusion of more non-pathological patches helped to improve the segmentation in the Cirrus and Topcon volumes, but also got a better performance than when no patches were dropped (*Run5*).\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.312  |  0.045  |  0.240  |\n",
    "| Spectralis |  0.559  |  0.728  |  0.518  |\n",
    "| Topcon     |  0.295  |  0.038  |  0.197  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.399  |  0.190  |  0.308  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run 8*\n",
    "- Repetion of Run 1, dropping 100% of the non-pathological patches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(\n",
    "    run_name=\"Run8\",\n",
    "    model_name=\"UNet\",\n",
    "    device=\"GPU\",\n",
    "    split=\"segmentation_fold_selection.csv\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_name=\"Adam\",\n",
    "    momentum=0.999,\n",
    "    weight_decay=0.0001,\n",
    "    gradient_clipping=1.0,\n",
    "    scheduler=False,     \n",
    "    patch_type=\"small\",\n",
    "    number_of_classes=4,\n",
    "    number_of_channels=1,\n",
    "    fold_test=1,\n",
    "    fold_val=2,\n",
    "    tuning=True,\n",
    "    patch_shape=(256,128), \n",
    "    n_pos=12, \n",
    "    n_neg=0, \n",
    "    pos=1, \n",
    "    neg=0,\n",
    "    amp=True,\n",
    "    assyncronous_patch_extraction=True,\n",
    "    patch_dropping=True,\n",
    "    drop_prob=1.,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_logs import plot_logs\n",
    "\n",
    "plot_logs(imgs_folder=\"unet_preliminary_imgs\", run_name=\"Run8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The results in training achieved a significantly higher loss due to a higher quantity of non-pathological scans when compared to the previous models. \n",
    "\n",
    "![Training Error in Run8](./unet_preliminary_imgs/Run8_training_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_model import test_model\n",
    "\n",
    "test_model(\n",
    "    fold_test=2,\n",
    "    model_name=\"UNet\",\n",
    "    weights_name=\"Run8_UNet_best_model.pth\",\n",
    "    number_of_channels=1,\n",
    "    number_of_classes=4,\n",
    "    device_name=\"GPU\",\n",
    "    batch_size=1,\n",
    "    patch_type=\"small\",\n",
    "    save_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The Dice coefficient was calculated to each of the classes in each slice. Also, the Dice coefficient of each class in each volume and in each vendor was calculated. The results can be seen in the following CSV files: [slice](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run8_slice_dice.csv), [vendor](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run8_vendor_dice.csv), [volume](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run8_volume_dice.csv), and [class](https://github.com/davidterroso/D/tree/bfc22c6e7872ab9f87a12d49ca6839bf999e5dee/results/Run8_class_dice.csv). Improved IRF score but worsened SRF and PED performances.\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "| Vendor     |   IRF   |   SRF   |   PED   |\n",
    "| ---------- | ------- | ------- | ------- |\n",
    "| Cirrus     |  0.354  |  0.059  |  0.177  |\n",
    "| Spectralis |  0.500  |  0.671  |  0.319  |\n",
    "| Topcon     |  0.431  |  0.039  |  0.052  |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    "|   IRF   |   SRF   |   PED   |\n",
    "| ------- | ------- | ------- |\n",
    "|  0.427  |  0.170  |  0.142  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall Testing Results** <a id=\"overallresults\"></a>\n",
    "\n",
    "**Runs**\n",
    "- *Run 1*: 100 Epochs, assyncronous patch extraction, and 75% drop percentage\n",
    "- *Run 2*: 200 Epochs, assyncronous patch extraction, and 75% drop percentage\n",
    "- *Run 3*: 100 Epochs, syncronous patch extraction, and 0% drop percentage\n",
    "- *Run 4*: 100 Epochs, syncronous patch extraction, and 75% drop percentage\n",
    "- *Run 5*: 100 Epochs, assyncronous patch extraction, and 0% drop percentage\n",
    "- *Run 6*: 100 Epochs, assyncronous patch extraction, and 50% drop percentage\n",
    "- *Run 7*: 100 Epochs, assyncronous patch extraction, and 25% drop percentage\n",
    "- *Run 8*: 100 Epochs, assyncronous patch extraction, and 100% drop percentage\n",
    "\n",
    "\n",
    "**Dice per vendor**\n",
    "\n",
    "|          |              |     **IRF**    |              |             |    **SRF**     |              |             |    **PED**     |              |\n",
    "| :------: | :----------: | :------------: | :----------: | :---------: | :------------: | :----------: | :---------: | :------------: | :----------: |\n",
    "| **Run**  |  **Cirrus**  | **Spectralis** |  **Topcon**  |  **Cirrus** | **Spectralis** |  **Topcon**  |  **Cirrus** | **Spectralis** |  **Topcon**  |\n",
    "|   Run1   |    0.235     |     0.515      |    0.401     |  **0.176**  |     0.706      |  **0.100**   |    0.191    |     0.470      |    0.123     |\n",
    "|   Run2   |    0.156     |     0.463      |    0.462     |    0.106    |     0.660      |    0.014     |    0.177    |     0.411      |    0.139     |\n",
    "|   Run3   |    0.230     |     0.582      |    0.371     |    0.103    |     0.751      |    0.004     |    0.217    |     0.491      |    0.226     |\n",
    "|   Run4   |    0.191     |     0.513      |  **0.530**   |    0.105    |     0.690      |    0.075     |    0.193    |     0.436      |  **0.233**   |\n",
    "|   Run5   |    0.133     |     0.592      |    0.274     |    0.069    |     0.703      |    0.014     |    0.183    |     0.428      |    0.229     |\n",
    "|   Run6   |    0.093     |   **0.602**    |    0.136     |    0.065    |   **0.793**    |    0.033     |    0.190    |     0.313      |    0.087     |\n",
    "|   Run7   |    0.312     |     0.559      |    0.295     |    0.045    |     0.728      |    0.038     |  **0.240**  |   **0.518**    |    0.197     |\n",
    "|   Run8   |  **0.354**   |     0.500      |    0.431     |    0.059    |     0.670      |    0.039     |    0.177    |     0.319      |    0.052     |\n",
    "\n",
    "**Dice per fluid**\n",
    "\n",
    " Run  |   IRF   |   SRF   |   PED   |\n",
    " :--: | :-----: | :-----: | :-----: |\n",
    " Run1 |  0.355  |**0.371**|  0.223  |\n",
    " Run2 |  0.272  |  0.218  |  0.215  |\n",
    " Run3 |  0.363  |  0.275  |  0.306  |\n",
    " Run4 |  0.343  |  0.191  |  0.278  |\n",
    " Run5 |  0.259  |  0.189  |  0.251  |\n",
    " Run6 |  0.222  |  0.165  |  0.180  |\n",
    " Run7 |  0.399  |  0.190  |**0.308**|\n",
    " Run8 |**0.427**|  0.171  |  0.142  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
